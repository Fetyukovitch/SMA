{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dddc7eb7-b461-4d53-bcb1-fc536b3b084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Quarterly EPS</th>\n",
       "      <th>Yearly EPS</th>\n",
       "      <th>PE</th>\n",
       "      <th>Quarterly Revenue</th>\n",
       "      <th>Quarterly Net Income</th>\n",
       "      <th>Quarterly Operating Margin</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Flag</th>\n",
       "      <th>PCE</th>\n",
       "      <th>VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>47.847500</td>\n",
       "      <td>48.152500</td>\n",
       "      <td>47.787498</td>\n",
       "      <td>48.070000</td>\n",
       "      <td>45.939304</td>\n",
       "      <td>67644400.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.866667</td>\n",
       "      <td>61.137</td>\n",
       "      <td>13.822</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>251.989</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13900.2</td>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>48.105000</td>\n",
       "      <td>48.220001</td>\n",
       "      <td>47.610001</td>\n",
       "      <td>47.674999</td>\n",
       "      <td>45.561821</td>\n",
       "      <td>86553600.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.728070</td>\n",
       "      <td>61.137</td>\n",
       "      <td>13.822</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>251.989</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13900.2</td>\n",
       "      <td>12.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>47.887501</td>\n",
       "      <td>47.892502</td>\n",
       "      <td>47.555000</td>\n",
       "      <td>47.700001</td>\n",
       "      <td>45.585724</td>\n",
       "      <td>86440400.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.736842</td>\n",
       "      <td>61.137</td>\n",
       "      <td>13.822</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>251.989</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13900.2</td>\n",
       "      <td>12.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-06-15</td>\n",
       "      <td>47.507500</td>\n",
       "      <td>47.540001</td>\n",
       "      <td>47.064999</td>\n",
       "      <td>47.209999</td>\n",
       "      <td>45.117435</td>\n",
       "      <td>246876800.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.564912</td>\n",
       "      <td>61.137</td>\n",
       "      <td>13.822</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>251.989</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13900.2</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>46.970001</td>\n",
       "      <td>47.305000</td>\n",
       "      <td>46.799999</td>\n",
       "      <td>47.185001</td>\n",
       "      <td>45.093540</td>\n",
       "      <td>73939600.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.556141</td>\n",
       "      <td>61.137</td>\n",
       "      <td>13.822</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>251.989</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13900.2</td>\n",
       "      <td>12.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>178.440002</td>\n",
       "      <td>181.210007</td>\n",
       "      <td>177.320007</td>\n",
       "      <td>177.820007</td>\n",
       "      <td>177.820007</td>\n",
       "      <td>61944600.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>5.89</td>\n",
       "      <td>30.190154</td>\n",
       "      <td>94.836</td>\n",
       "      <td>24.160</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>304.127</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23096.9</td>\n",
       "      <td>13.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>177.899994</td>\n",
       "      <td>180.839996</td>\n",
       "      <td>177.460007</td>\n",
       "      <td>180.570007</td>\n",
       "      <td>180.570007</td>\n",
       "      <td>50214900.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>5.89</td>\n",
       "      <td>30.657047</td>\n",
       "      <td>94.836</td>\n",
       "      <td>24.160</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>304.127</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23096.9</td>\n",
       "      <td>13.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2023-06-09</td>\n",
       "      <td>181.500000</td>\n",
       "      <td>182.229996</td>\n",
       "      <td>180.630005</td>\n",
       "      <td>180.960007</td>\n",
       "      <td>180.960007</td>\n",
       "      <td>48870700.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>5.89</td>\n",
       "      <td>30.723261</td>\n",
       "      <td>94.836</td>\n",
       "      <td>24.160</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>304.127</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23096.9</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>181.270004</td>\n",
       "      <td>183.889999</td>\n",
       "      <td>180.970001</td>\n",
       "      <td>183.789993</td>\n",
       "      <td>183.789993</td>\n",
       "      <td>54274900.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>5.89</td>\n",
       "      <td>31.203734</td>\n",
       "      <td>94.836</td>\n",
       "      <td>24.160</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>304.127</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23096.9</td>\n",
       "      <td>15.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>182.800003</td>\n",
       "      <td>184.149994</td>\n",
       "      <td>182.440002</td>\n",
       "      <td>183.309998</td>\n",
       "      <td>183.309998</td>\n",
       "      <td>54868400.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>5.89</td>\n",
       "      <td>31.122241</td>\n",
       "      <td>94.836</td>\n",
       "      <td>24.160</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>304.127</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23096.9</td>\n",
       "      <td>14.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2018-06-12   47.847500   48.152500   47.787498   48.070000   45.939304   \n",
       "1    2018-06-13   48.105000   48.220001   47.610001   47.674999   45.561821   \n",
       "2    2018-06-14   47.887501   47.892502   47.555000   47.700001   45.585724   \n",
       "3    2018-06-15   47.507500   47.540001   47.064999   47.209999   45.117435   \n",
       "4    2018-06-18   46.970001   47.305000   46.799999   47.185001   45.093540   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "1255 2023-06-07  178.440002  181.210007  177.320007  177.820007  177.820007   \n",
       "1256 2023-06-08  177.899994  180.839996  177.460007  180.570007  180.570007   \n",
       "1257 2023-06-09  181.500000  182.229996  180.630005  180.960007  180.960007   \n",
       "1258 2023-06-12  181.270004  183.889999  180.970001  183.789993  183.789993   \n",
       "1259 2023-06-13  182.800003  184.149994  182.440002  183.309998  183.309998   \n",
       "\n",
       "           Volume  Quarterly EPS  Yearly EPS         PE Quarterly Revenue  \\\n",
       "0      67644400.0           0.68        2.85  16.866667            61.137   \n",
       "1      86553600.0           0.68        2.85  16.728070            61.137   \n",
       "2      86440400.0           0.68        2.85  16.736842            61.137   \n",
       "3     246876800.0           0.68        2.85  16.564912            61.137   \n",
       "4      73939600.0           0.68        2.85  16.556141            61.137   \n",
       "...           ...            ...         ...        ...               ...   \n",
       "1255   61944600.0           1.52        5.89  30.190154            94.836   \n",
       "1256   50214900.0           1.52        5.89  30.657047            94.836   \n",
       "1257   48870700.0           1.52        5.89  30.723261            94.836   \n",
       "1258   54274900.0           1.52        5.89  31.203734            94.836   \n",
       "1259   54868400.0           1.52        5.89  31.122241            94.836   \n",
       "\n",
       "     Quarterly Net Income  Quarterly Operating Margin      CPI  Interest  \\\n",
       "0                  13.822                      0.2670  251.989    0.0170   \n",
       "1                  13.822                      0.2670  251.989    0.0170   \n",
       "2                  13.822                      0.2670  251.989    0.0190   \n",
       "3                  13.822                      0.2670  251.989    0.0190   \n",
       "4                  13.822                      0.2670  251.989    0.0190   \n",
       "...                   ...                         ...      ...       ...   \n",
       "1255               24.160                      0.2916  304.127    0.0508   \n",
       "1256               24.160                      0.2916  304.127    0.0508   \n",
       "1257               24.160                      0.2916  304.127    0.0508   \n",
       "1258               24.160                      0.2916  304.127    0.0508   \n",
       "1259               24.160                      0.2916  304.127    0.0508   \n",
       "\n",
       "      Flag      PCE    VIX  \n",
       "0      0.0  13900.2  12.34  \n",
       "1      0.0  13900.2  12.94  \n",
       "2      0.0  13900.2  12.12  \n",
       "3      0.0  13900.2  11.98  \n",
       "4      0.0  13900.2  12.31  \n",
       "...    ...      ...    ...  \n",
       "1255   0.0  23096.9  13.94  \n",
       "1256   0.0  23096.9  13.65  \n",
       "1257   0.0  23096.9  13.83  \n",
       "1258   0.0  23096.9  15.01  \n",
       "1259   0.0  23096.9  14.61  \n",
       "\n",
       "[1260 rows x 18 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data(input_date):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    import requests\n",
    "    import datetime\n",
    "    import yfinance as yf\n",
    "    \n",
    "    data= yf.download(\"AAPL\",\"2018-06-12\", input_date)\n",
    "    stock_info_df= pd.DataFrame(data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']])\n",
    "    stock_info_df = stock_info_df.sort_values(by='Date',ascending=True)\n",
    "    stock_info_df.reset_index(drop=False, inplace=True)\n",
    "    stock_info_df['Date'] = pd.to_datetime(stock_info_df['Date'])\n",
    "\n",
    "\n",
    "\n",
    "    # Define the start and end dates\n",
    "    start_date = '2018-01-01'\n",
    "    end_date = input_date\n",
    "\n",
    "    # Create an empty list to store the dates\n",
    "    dates = []\n",
    "\n",
    "    # Iterate over each month\n",
    "    for year in range(pd.to_datetime(start_date).year, pd.to_datetime(end_date).year + 1):\n",
    "        for month in range(1, 13):\n",
    "            # Create the start and end dates for the current month\n",
    "            month_start = f\"{year}-{month:02d}-01\"\n",
    "            month_end = pd.to_datetime(month_start) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "            # Generate a range of dates for the current month\n",
    "            month_dates = pd.date_range(start=month_start, end=month_end, freq='D')\n",
    "\n",
    "            # Append the dates to the list\n",
    "            dates.extend(month_dates)\n",
    "\n",
    "    # Create a DataFrame from the list of dates\n",
    "    all_days_df = pd.DataFrame({'Date': dates})\n",
    "\n",
    "    # replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
    "    url = 'https://www.alphavantage.co/query?function=CPI&interval=monthly&apikey=WIB4EPEUDA7KJFOG'\n",
    "    r = requests.get(url)\n",
    "    data_str = r.text\n",
    "\n",
    "    # Parse the string into a dictionary\n",
    "    data = json.loads(data_str)\n",
    "\n",
    "    df_CPI = pd.DataFrame(data['data'])\n",
    "    df_CPI.rename(columns={'date': 'Date'}, inplace=True)\n",
    "    df_CPI.rename(columns={'value': 'CPI'}, inplace=True)\n",
    "    df_CPI = df_CPI[df_CPI['Date'] >= '2018-05-12']\n",
    "    df_CPI = df_CPI.sort_values(by='Date',ascending=True)\n",
    "    df_CPI.reset_index(drop=True, inplace=True)\n",
    "    df_CPI['Date'] = pd.to_datetime(df_CPI['Date'])\n",
    "\n",
    "    df_CPI_daily = all_days_df.merge(df_CPI, on='Date', how='left')\n",
    "\n",
    "    df_CPI_daily['CPI'] = df_CPI_daily['CPI'].fillna(method='ffill')\n",
    "\n",
    "    df_CPI_daily = df_CPI_daily[df_CPI_daily['Date'] >= '2018-06-12']\n",
    "    df_CPI_daily = df_CPI_daily[df_CPI_daily['Date'] <= input_date]\n",
    "\n",
    "    # replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
    "    url = 'https://www.alphavantage.co/query?function=FEDERAL_FUNDS_RATE&interval=daily&apikey=WIB4EPEUDA7KJFOG'\n",
    "    r = requests.get(url)\n",
    "    data_str = r.text\n",
    "\n",
    "    # Parse the string into a dictionary\n",
    "    data = json.loads(data_str)\n",
    "\n",
    "    df_interest = pd.DataFrame(data['data'])\n",
    "    df_interest.rename(columns={'date': 'Date'}, inplace=True)\n",
    "    df_interest.rename(columns={'value': 'Interest'}, inplace=True)\n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_interest['Date'] = pd.to_datetime(df_interest['Date'])\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df_interest = df_interest[df_interest['Date'] >= '2018-06-12']\n",
    "    df_CPI_daily = df_CPI_daily[df_CPI_daily['Date'] <= input_date]\n",
    "    df_interest = df_interest.sort_values(by='Date',ascending=True)\n",
    "    # Convert 'Interest' column to numeric\n",
    "    df_interest['Interest'] = pd.to_numeric(df_interest['Interest'], errors='coerce')\n",
    "    df_interest['Interest'] = df_interest['Interest'] / 100\n",
    "    df_interest.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Convert the 'Date' column in df_interest to datetime data type\n",
    "    df_interest['Date'] = pd.to_datetime(df_interest['Date'])\n",
    "    # Convert the 'Date' column in df_CPI to datetime data type\n",
    "\n",
    "    # Perform the merge\n",
    "    df_interest_cpi = df_CPI_daily.merge(df_interest, on='Date', how='left')\n",
    "\n",
    "    # Specify the file path of the CSV file\n",
    "    csv_file_path = '/Users/emanuelbayat/code/Fetyukovitch/SMA/raw_data/AAPL_Financials.csv'\n",
    "\n",
    "    # Read the CSV file using pandas\n",
    "    aapl_finances_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    #12th junen 2018\n",
    "    aapl_finances_df = aapl_finances_df.drop(aapl_finances_df[aapl_finances_df.index > 22].index)\n",
    "\n",
    "    aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'].str.replace('$', '')\n",
    "    aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'].str.replace('$', '')\n",
    "    aapl_finances_df['Apple Quarterly EPS'] = aapl_finances_df['Apple Quarterly EPS'].str.replace('$','')\n",
    "    aapl_finances_df['Apple Quarterly Operating Margin'] = aapl_finances_df['Apple Quarterly Operating Margin'].str.replace('%','')\n",
    "    # Convert the column to numeric type\n",
    "    aapl_finances_df['Apple Quarterly Operating Margin'] = pd.to_numeric(aapl_finances_df['Apple Quarterly Operating Margin'], errors='coerce')\n",
    "    aapl_finances_df['Apple Quarterly Operating Margin'] = aapl_finances_df['Apple Quarterly Operating Margin'] / 100\n",
    "\n",
    "    aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'].str.replace(',', '.')\n",
    "    aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'].str.replace(',', '.')\n",
    "    aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'].str.replace(',', '.')\n",
    "    aapl_finances_df=aapl_finances_df.sort_values(by='Dates',ascending=True)\n",
    "    aapl_finances_df.reset_index(drop=True, inplace=True)\n",
    "    aapl_finances_df.rename(columns={'Dates': 'Date'}, inplace=True)\n",
    "\n",
    "    # Assuming the DataFrame is already defined as \"aapl_finances_df\"\n",
    "\n",
    "    # Convert the \"Date\" column to datetime format\n",
    "    aapl_finances_df['Date'] = pd.to_datetime(aapl_finances_df['Date'])\n",
    "\n",
    "    # Add a new column \"Day of Week\" representing the day of the week as a number (0-6)\n",
    "    aapl_finances_df['Day of Week'] = aapl_finances_df['Date'].dt.weekday\n",
    "\n",
    "    # Assuming the DataFrame is already defined as \"aapl_finances_df\"\n",
    "\n",
    "    # Convert the \"Date\" column to datetime format\n",
    "    aapl_finances_df['Date'] = pd.to_datetime(aapl_finances_df['Date'])\n",
    "\n",
    "    # Add a new column \"Day of Week\" representing the day of the week as a number (0-6)\n",
    "    aapl_finances_df['Day of Week'] = aapl_finances_df['Date'].dt.weekday\n",
    "\n",
    "    # Subtract the corresponding value from the day if \"Day of Week\" is greater than 4\n",
    "    aapl_finances_df.loc[aapl_finances_df['Day of Week'] > 4, 'Date'] -= pd.to_timedelta(aapl_finances_df['Day of Week'] - 4, unit='D')\n",
    "\n",
    "    # Update the \"Day of Week\" column to represent the correct day of the week (4 for values > 4)\n",
    "    aapl_finances_df['Day of Week'] = aapl_finances_df['Date'].dt.weekday\n",
    "\n",
    "    aapl_finances_df = aapl_finances_df.drop('Day of Week', axis=1)\n",
    "\n",
    "    df_EPS=pd.DataFrame(aapl_finances_df[['Date', 'Apple Quarterly EPS']])\n",
    "\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_new\n",
    "    stock_info_df['Date'] = pd.to_datetime(stock_info_df['Date'])\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_EPS\n",
    "    df_EPS['Date'] = pd.to_datetime(df_EPS['Date'])\n",
    "\n",
    "    # Merge df_EPS onto df_new using left merge\n",
    "    merged_financial_df = stock_info_df.merge(df_EPS, on='Date', how='left')\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_new\n",
    "    stock_info_df['Date'] = pd.to_datetime(stock_info_df['Date'])\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_EPS\n",
    "    df_EPS['Date'] = pd.to_datetime(df_EPS['Date'])\n",
    "\n",
    "    # Merge df_EPS onto df_new using left merge\n",
    "    merged_financial_df = stock_info_df.merge(df_EPS, on='Date', how='left')\n",
    "\n",
    "    # Find missing date entries\n",
    "    missing_dates = df_EPS[~df_EPS['Date'].isin(merged_financial_df['Date'])]\n",
    "\n",
    "    # Create a new DataFrame with missing dates\n",
    "    df_new = pd.DataFrame(missing_dates)\n",
    "\n",
    "    # Add the entries from df_new to merged_financial_df\n",
    "    merged_financial_df = pd.concat([merged_financial_df, df_new])\n",
    "\n",
    "    # Sort the DataFrame by the \"Date\" column\n",
    "    merged_financial_df = merged_financial_df.sort_values('Date')\n",
    "\n",
    "    # Reset the index\n",
    "    merged_financial_df = merged_financial_df.reset_index(drop=True)\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    merged_financial_df['Apple Quarterly EPS'] = merged_financial_df['Apple Quarterly EPS'].fillna(method='ffill')\n",
    "\n",
    "    merged_financial_df = merged_financial_df.sort_values('Date', ascending=True)\n",
    "    merged_financial_df['Yearly EPS'] = np.nan\n",
    "\n",
    "    start_date = pd.to_datetime('2018-06-12')\n",
    "\n",
    "    merged_financial_df['Apple Quarterly EPS'] = merged_financial_df['Apple Quarterly EPS'].astype(float)\n",
    "\n",
    "    for index, row in merged_financial_df.iterrows():\n",
    "        current_date = row['Date']\n",
    "\n",
    "        if current_date >= start_date:\n",
    "            previous_values = merged_financial_df[(merged_financial_df['Date'] < current_date) & (~merged_financial_df['Apple Quarterly EPS'].isna())]['Apple Quarterly EPS'].unique()\n",
    "            if len(previous_values) >= 3:\n",
    "                previous_values = previous_values[-3:]\n",
    "                yearly_eps = row['Apple Quarterly EPS'] + previous_values.sum()\n",
    "                merged_financial_df.at[index, 'Yearly EPS'] = yearly_eps\n",
    "\n",
    "\n",
    "    # Convert \"Date\" column to datetime format\n",
    "    merged_financial_df['Date'] = pd.to_datetime(merged_financial_df['Date'])\n",
    "\n",
    "    # Calculate the PE ratio\n",
    "    merged_financial_df['PE'] = merged_financial_df['Close'] / merged_financial_df['Yearly EPS']\n",
    "\n",
    "    # Filter data from the date 2018-06-29 onwards\n",
    "    merged_financial_df = merged_financial_df[merged_financial_df['Date'] >= '2018-06-12']\n",
    "\n",
    "    merged_financial_df = merged_financial_df.copy()\n",
    "    #merged_financial_df.rename(columns={'Apple Quarterly EPS': 'Quarterly EPS'}, inplace=True)\n",
    "\n",
    "    aapl_finances_df = all_days_df.merge(aapl_finances_df, on='Date', how='left')\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'].fillna(method='ffill')\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'].fillna(method='ffill')\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    aapl_finances_df['Apple Quarterly Operating Margin'] = aapl_finances_df['Apple Quarterly Operating Margin'].fillna(method='ffill')\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    aapl_finances_df['Apple Quarterly EPS'] = aapl_finances_df['Apple Quarterly EPS'].fillna(method='ffill')\n",
    "\n",
    "    # Filter the DataFrame to keep only the entries on or after '2018-06-12'\n",
    "    aapl_finances_df = aapl_finances_df[aapl_finances_df['Date'] >= '2018-06-12']\n",
    "    aapl_finances_df = aapl_finances_df[aapl_finances_df['Date'] <= input_date]\n",
    "    # Reset the index of the DataFrame\n",
    "    aapl_finances_df = aapl_finances_df.reset_index(drop=True)\n",
    "\n",
    "    df_final = merged_financial_df.merge(aapl_finances_df, on='Date', how='left')\n",
    "\n",
    "    df_final = df_final.merge(df_interest_cpi, on='Date', how='left')\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Sorted dates without duplicates\n",
    "    sorted_dates = [\n",
    "        '2018-07-12', '2018-09-21', '2018-10-26', '2018-10-30', '2018-11-07',\n",
    "        '2019-03-18', '2019-03-19', '2019-03-20', '2019-03-25', '2019-05-21',\n",
    "        '2019-05-28', '2019-06-03', '2019-06-04', '2019-06-05', '2019-06-06',\n",
    "        '2019-06-07', '2019-07-09', '2019-08-20', '2019-09-10', '2019-09-20',\n",
    "        '2019-09-25', '2020-03-18', '2020-04-24', '2020-05-04', '2020-06-22',\n",
    "        '2020-06-23', '2020-06-24', '2020-06-25', '2020-06-26', '2020-08-04',\n",
    "        '2020-09-15', '2020-09-18', '2020-10-13', '2020-10-23', '2020-11-10',\n",
    "        '2020-11-13', '2020-11-16', '2020-11-17', '2020-12-15', '2021-04-20',\n",
    "        '2021-04-30', '2021-05-21', '2021-07-13', '2021-09-14', '2021-09-24',\n",
    "        '2021-10-08', '2021-10-18', '2021-10-26', '2021-11-01', '2022-03-08',\n",
    "        '2022-03-18', '2022-06-06', '2022-06-07', '2022-06-08', '2022-06-09',\n",
    "        '2022-06-10', '2022-06-24', '2022-07-15', '2022-09-07', '2022-09-16',\n",
    "        '2022-09-23', '2022-10-07', '2022-10-26', '2022-11-04', '2023-01-24',\n",
    "        '2023-02-03', '2023-06-05' , '2023-06-06'\n",
    "    ]\n",
    "\n",
    "    # Create DataFrame with dates and a column with all entries as \"1\"\n",
    "    flag_df = pd.DataFrame({'Date': sorted_dates})\n",
    "    flag_df['Flag'] = 1\n",
    "\n",
    "    # Display the DataFrame\n",
    "    event_dates = flag_df[\"Date\"].to_list()\n",
    "\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_EPS\n",
    "    flag_df['Date'] = pd.to_datetime(flag_df['Date'])\n",
    "\n",
    "    # Merge df_EPS onto df_new using left merge\n",
    "    df_final = df_final.merge(flag_df, on='Date', how='left')\n",
    "    df_final['Flag'] = df_final['Flag'].fillna(0)\n",
    "\n",
    "\n",
    "    # Replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
    "    url = 'https://api.stlouisfed.org/fred/series/observations'\n",
    "    params = {\n",
    "        'series_id': 'PCE',\n",
    "        'api_key': '55f5943ac55f44a9be924f4751900af3',\n",
    "        'file_type': 'json',\n",
    "        'observation_start': '2018-06-12',\n",
    "        'observation_end': input_date\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params)\n",
    "    data = r.json()\n",
    "    # Extract the 'observations' list from the JSON data\n",
    "    observations = data['observations']\n",
    "    # Create a pandas DataFrame with only 'date' and 'value' columns\n",
    "    PCE_monthly_df = pd.DataFrame(observations, columns=['date', 'value'])\n",
    "    # Convert 'value' column to float\n",
    "    PCE_monthly_df['value'] = PCE_monthly_df['value'].astype(float)\n",
    "    PCE_monthly_df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "    PCE_monthly_df.rename(columns={'value': 'PCE'}, inplace=True)\n",
    "\n",
    "    # Calculate the slope using linear regression\n",
    "    values = PCE_monthly_df['PCE'].values\n",
    "    time = np.arange(len(values))\n",
    "    slope, _ = np.polyfit(time, values, 1)\n",
    "\n",
    "    # Get the last date in the DataFrame\n",
    "    last_date = datetime.datetime.strptime(PCE_monthly_df['Date'].iloc[-1], '%Y-%m-%d')\n",
    "\n",
    "    # Generate the next two months' dates (first day of the month)\n",
    "    next_date_1 = datetime.datetime(last_date.year, last_date.month + 1, 1)\n",
    "    next_date_2 = datetime.datetime(last_date.year, last_date.month + 2, 1)\n",
    "\n",
    "    # Create a DataFrame with the extrapolated dates\n",
    "    extrapolated_dates = pd.DataFrame({'Date': [next_date_1, next_date_2]})\n",
    "\n",
    "    # Extrapolate the next two months' values based on the calculated slope\n",
    "    extrapolated_values = values[-1] + slope * (len(values) + np.arange(2))\n",
    "\n",
    "    # Add the extrapolated values to the DataFrame\n",
    "    extrapolated_dates['PCE'] = extrapolated_values\n",
    "\n",
    "    # Concatenate the extrapolated dates DataFrame with the original DataFrame\n",
    "    extrapolated_df = pd.concat([PCE_monthly_df, extrapolated_dates])\n",
    "\n",
    "    extrapolated_df['Date'] = pd.to_datetime(extrapolated_df['Date']).dt.strftime('%Y-%m-%d')\n",
    "    # Round the values in the 'PCE' column to one decimal point\n",
    "    extrapolated_df['PCE'] = extrapolated_df['PCE'].round(1)\n",
    "\n",
    "    extrapolated_df = extrapolated_df.reset_index(drop=True)\n",
    "\n",
    "    # Convert 'Date' column to datetime format\n",
    "    extrapolated_df['Date'] = pd.to_datetime(extrapolated_df['Date'])\n",
    "\n",
    "    # Create a new DataFrame with a 'Date' column containing all dates from 2018-06-01 to 2023-06-30\n",
    "    date_range = pd.date_range(start='2018-06-01', end=input_date, freq='D')\n",
    "    new_2_df = pd.DataFrame({'Date': date_range})\n",
    "\n",
    "    # Convert 'Date' column to datetime format\n",
    "    new_2_df['Date'] = pd.to_datetime(new_2_df['Date'])\n",
    "\n",
    "    # Merge the new DataFrame with 'extrapolated_df' based on the 'Date' column\n",
    "    PCE_daily_df = pd.merge(new_2_df, extrapolated_df, on='Date', how='left')\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    PCE_daily_df['PCE'] = PCE_daily_df['PCE'].fillna(method='ffill')\n",
    "\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_new\n",
    "    PCE_daily_df['Date'] = pd.to_datetime(PCE_daily_df['Date'])\n",
    "\n",
    "    # Merge df_EPS onto df_new using left merge\n",
    "    df_final = df_final.merge(PCE_daily_df, on='Date', how='left')\n",
    "\n",
    "\n",
    "    # Replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
    "    url = 'https://api.stlouisfed.org/fred/series/observations'\n",
    "    params = {\n",
    "        'series_id': 'VIXCLS',\n",
    "        'api_key': '55f5943ac55f44a9be924f4751900af3',\n",
    "        'file_type': 'json',\n",
    "        'observation_start': '2018-06-12',\n",
    "        'observation_end': input_date\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params)\n",
    "    data = r.json()\n",
    "\n",
    "    # Extract the 'observations' list from the JSON data\n",
    "    observations = data['observations']\n",
    "\n",
    "    # Create a pandas DataFrame with only 'date' and 'value' columns\n",
    "    vix_daily_df = pd.DataFrame(observations, columns=['date', 'value'])\n",
    "\n",
    "    vix_daily_df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "    vix_daily_df.rename(columns={'value': 'VIX'}, inplace=True)\n",
    "    \n",
    "    # Convert \"Date\" column to datetime in df_new\n",
    "    vix_daily_df['Date'] = pd.to_datetime(vix_daily_df['Date'])\n",
    "\n",
    "    # Merge df_EPS onto df_new using left merge\n",
    "    df_final = df_final.merge(vix_daily_df, on='Date', how='left')\n",
    "\n",
    "    symbol = \"^VIX\"\n",
    "    data = yf.Ticker(symbol)\n",
    "    historical_data = data.history(period=\"5d\")  # Fetching data for a longer period (5 days)\n",
    "    previous_close = historical_data[\"Close\"].iloc[-2]  # Using iloc to retrieve the second-to-last value\n",
    "    df_final.loc[df_final.index[-1], \"VIX\"] = previous_close\n",
    "\n",
    "    df_final['Interest'] = df_final['Interest'].fillna(method='ffill')\n",
    "\n",
    "    df_final = df_final.drop('Apple Quarterly EPS_y', axis=1)\n",
    "    df_final.rename(columns={'Apple Quarterly EPS_x': 'Quarterly EPS'}, inplace=True)\n",
    "    df_final.rename(columns={'Apple Quarterly Revenue\\n(Millions of US $)': 'Quarterly Revenue'}, inplace=True)\n",
    "    df_final.rename(columns={'Apple Quarterly Net Income\\n(Millions of US $)': 'Quarterly Net Income'}, inplace=True)\n",
    "    df_final.rename(columns={'Apple Quarterly Operating Margin': 'Quarterly Operating Margin'}, inplace=True)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "input_date = '2023-06-14'  # Replace with your desired date\n",
    "output_data = get_data(input_date)\n",
    "\n",
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b58bebf-c108-480f-bb63-5aca59e7e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56e29af0-d53f-4fa4-9b08-8ab579a2ae30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 15:57:53.584806: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x29bd65360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "Predicted probability:  0.5007513\n",
      "Predicted direction:  Increase\n",
      "Price from the day before the prediction:  183.30999755859375\n",
      "('buy', 0.5007513, 183.30999755859375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 15:58:00.338350: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    }
   ],
   "source": [
    "# !pip install focal_loss\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Attention, Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from focal_loss import BinaryFocalLoss\n",
    "\n",
    "es = EarlyStopping(patience=4, monitor='loss')\n",
    "set_path = \"/content/sample_data/\"\n",
    "\n",
    "\n",
    "def predictor(date, train_size=250):\n",
    "                                    \n",
    "    data = get_data(input_date) \n",
    "    \n",
    "    \"\"\"\n",
    "    Optional drops\n",
    "    \"\"\"\n",
    "    \n",
    "    # data = data.drop(columns=['Apple Quarterly Operating Margin', 'Apple Quarterly Net Income\\n(Millions of US $)'])\n",
    "    \n",
    "    # Set 'Adj Close' column as y and remaining columns as features X\n",
    "    X = data.drop(columns=['Date', 'Adj Close', 'PCE', 'Flag'])\n",
    "    X = X.astype(float)\n",
    "    y = data['Adj Close']\n",
    "    \n",
    "    y_diff = y.diff().dropna()\n",
    "    \n",
    "    # Normalize features to between 0 and 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Normalize target variable separately\n",
    "    y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    y_diff_scaled = y_scaler.fit_transform(np.array(y_diff).reshape(-1, 1))\n",
    "    \n",
    "    # Function to create sequences\n",
    "    def create_sequences(X, y, time_steps=5):\n",
    "      Xs, ys = [], []\n",
    "      for i in range(len(X) - (time_steps)):\n",
    "            Xs.append(X[i:(i + time_steps)])\n",
    "            ys.append(y[i + time_steps - 1])\n",
    "    \n",
    "      return np.array(Xs), np.array(ys)\n",
    "    \n",
    "    \n",
    "    y_bin = np.where(y.diff() > 0, 1, 0)[1:]  # Shift by one to exclude the first NaN value from diff()\n",
    "    y_shift = np.array(y)[1:]\n",
    "    # Normalize target variable separately\n",
    "    y_bin_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    y_bin_scaled = y_bin_scaler.fit_transform(np.array(y_bin).reshape(-1, 1))\n",
    "    \n",
    "    time_steps = 10  # change to whatever value you want\n",
    "    X_seq, y_bin_seq = create_sequences(X_scaled, y_bin_scaled, time_steps)\n",
    "    X_seq_o, y_bin_o = create_sequences(X, y_shift, time_steps)\n",
    "    \n",
    "    # Loop from train_size to len(X_seq) + 1\n",
    "    # Training Data\n",
    "    X_train = X_seq  # Exclude the last row from X\n",
    "    y_train = y_bin_seq  # Exclude the last row from Y\n",
    "    \n",
    "    # Test Data (for prediction)\n",
    "    X_test = np.array(X_scaled[-time_steps:]).reshape(1, X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Build the LSTM model\n",
    "    sequence_length = X_train.shape[1]\n",
    "    feature_length = X_train.shape[2]\n",
    "    \n",
    "    # Define the layers\n",
    "    inputs = Input(shape=(sequence_length, feature_length))\n",
    "    \n",
    "    lstm_out, hidden_h, hidden_c = LSTM(64, return_sequences=True, return_state=True)(inputs)\n",
    "    attention = Attention()([lstm_out, lstm_out])\n",
    "    lstm_out2 = LSTM(64, return_sequences=True)(attention)\n",
    "    lstm_out3 = LSTM(32)(lstm_out2)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(lstm_out3)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=0, callbacks=[es])\n",
    "    \n",
    "    predicted_prob = model.predict(X_test)[0][0]  # Probability of price increase\n",
    "    predicted_bin = np.where(predicted_prob > 0.485, 1, 0)  # Binary prediction\n",
    "    \n",
    "    # The actual price direction\n",
    "    \n",
    "    price_day_before = y.iloc[-1]\n",
    "    \n",
    "    print(\"Predicted probability: \", predicted_prob)\n",
    "    print(\"Predicted direction: \", \"Increase\" if predicted_bin else \"Decrease\")\n",
    "    \n",
    "    print(\"Price from the day before the prediction: \", price_day_before)\n",
    "    \n",
    "    if predicted_bin:\n",
    "      pred_action = 'buy'\n",
    "    else:\n",
    "      pred_action = 'sell'\n",
    "    return pred_action, predicted_prob, price_day_before\n",
    "\n",
    "\n",
    "print(predictor(\"2023-06-14\"))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625afd6-9b33-4412-a125-8d16d8edd653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
