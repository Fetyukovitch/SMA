{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da444a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Quarterly EPS</th>\n",
       "      <th>Yearly EPS</th>\n",
       "      <th>PE</th>\n",
       "      <th>Quarterly Revenue</th>\n",
       "      <th>Quarterly Net Income</th>\n",
       "      <th>Quarterly Operating Margin</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Flag</th>\n",
       "      <th>PCE</th>\n",
       "      <th>VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-12</td>\n",
       "      <td>47.847500</td>\n",
       "      <td>48.152500</td>\n",
       "      <td>47.787498</td>\n",
       "      <td>48.070000</td>\n",
       "      <td>45.939312</td>\n",
       "      <td>67644400.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.866667</td>\n",
       "      <td>61.137</td>\n",
       "      <td>13.822</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>251.989</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13900.2</td>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>48.105000</td>\n",
       "      <td>48.220001</td>\n",
       "      <td>47.610001</td>\n",
       "      <td>47.674999</td>\n",
       "      <td>45.561829</td>\n",
       "      <td>86553600.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.728070</td>\n",
       "      <td>61.137</td>\n",
       "      <td>13.822</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>251.989</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13900.2</td>\n",
       "      <td>12.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>47.887501</td>\n",
       "      <td>47.892502</td>\n",
       "      <td>47.555000</td>\n",
       "      <td>47.700001</td>\n",
       "      <td>45.585712</td>\n",
       "      <td>86440400.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.736842</td>\n",
       "      <td>61.137</td>\n",
       "      <td>13.822</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>251.989</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13900.2</td>\n",
       "      <td>12.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-06-15</td>\n",
       "      <td>47.507500</td>\n",
       "      <td>47.540001</td>\n",
       "      <td>47.064999</td>\n",
       "      <td>47.209999</td>\n",
       "      <td>45.117432</td>\n",
       "      <td>246876800.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.564912</td>\n",
       "      <td>61.137</td>\n",
       "      <td>13.822</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>251.989</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13900.2</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>46.970001</td>\n",
       "      <td>47.305000</td>\n",
       "      <td>46.799999</td>\n",
       "      <td>47.185001</td>\n",
       "      <td>45.093536</td>\n",
       "      <td>73939600.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.556141</td>\n",
       "      <td>61.137</td>\n",
       "      <td>13.822</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>251.989</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13900.2</td>\n",
       "      <td>12.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>178.440002</td>\n",
       "      <td>181.210007</td>\n",
       "      <td>177.320007</td>\n",
       "      <td>177.820007</td>\n",
       "      <td>177.820007</td>\n",
       "      <td>61944600.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>5.89</td>\n",
       "      <td>30.190154</td>\n",
       "      <td>94.836</td>\n",
       "      <td>24.160</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>304.127</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23096.9</td>\n",
       "      <td>13.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>177.899994</td>\n",
       "      <td>180.839996</td>\n",
       "      <td>177.460007</td>\n",
       "      <td>180.570007</td>\n",
       "      <td>180.570007</td>\n",
       "      <td>50214900.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>5.89</td>\n",
       "      <td>30.657047</td>\n",
       "      <td>94.836</td>\n",
       "      <td>24.160</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>304.127</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23096.9</td>\n",
       "      <td>13.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2023-06-09</td>\n",
       "      <td>181.500000</td>\n",
       "      <td>182.229996</td>\n",
       "      <td>180.630005</td>\n",
       "      <td>180.960007</td>\n",
       "      <td>180.960007</td>\n",
       "      <td>48870700.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>5.89</td>\n",
       "      <td>30.723261</td>\n",
       "      <td>94.836</td>\n",
       "      <td>24.160</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>304.127</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23096.9</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>181.270004</td>\n",
       "      <td>183.889999</td>\n",
       "      <td>180.970001</td>\n",
       "      <td>183.789993</td>\n",
       "      <td>183.789993</td>\n",
       "      <td>54274900.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>5.89</td>\n",
       "      <td>31.203734</td>\n",
       "      <td>94.836</td>\n",
       "      <td>24.160</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>304.127</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23096.9</td>\n",
       "      <td>15.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>182.800003</td>\n",
       "      <td>184.149994</td>\n",
       "      <td>182.440002</td>\n",
       "      <td>183.309998</td>\n",
       "      <td>183.309998</td>\n",
       "      <td>54868400.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>5.89</td>\n",
       "      <td>31.122241</td>\n",
       "      <td>94.836</td>\n",
       "      <td>24.160</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>304.127</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23096.9</td>\n",
       "      <td>14.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2018-06-12   47.847500   48.152500   47.787498   48.070000   45.939312   \n",
       "1    2018-06-13   48.105000   48.220001   47.610001   47.674999   45.561829   \n",
       "2    2018-06-14   47.887501   47.892502   47.555000   47.700001   45.585712   \n",
       "3    2018-06-15   47.507500   47.540001   47.064999   47.209999   45.117432   \n",
       "4    2018-06-18   46.970001   47.305000   46.799999   47.185001   45.093536   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "1255 2023-06-07  178.440002  181.210007  177.320007  177.820007  177.820007   \n",
       "1256 2023-06-08  177.899994  180.839996  177.460007  180.570007  180.570007   \n",
       "1257 2023-06-09  181.500000  182.229996  180.630005  180.960007  180.960007   \n",
       "1258 2023-06-12  181.270004  183.889999  180.970001  183.789993  183.789993   \n",
       "1259 2023-06-13  182.800003  184.149994  182.440002  183.309998  183.309998   \n",
       "\n",
       "           Volume  Quarterly EPS  Yearly EPS         PE Quarterly Revenue  \\\n",
       "0      67644400.0           0.68        2.85  16.866667            61.137   \n",
       "1      86553600.0           0.68        2.85  16.728070            61.137   \n",
       "2      86440400.0           0.68        2.85  16.736842            61.137   \n",
       "3     246876800.0           0.68        2.85  16.564912            61.137   \n",
       "4      73939600.0           0.68        2.85  16.556141            61.137   \n",
       "...           ...            ...         ...        ...               ...   \n",
       "1255   61944600.0           1.52        5.89  30.190154            94.836   \n",
       "1256   50214900.0           1.52        5.89  30.657047            94.836   \n",
       "1257   48870700.0           1.52        5.89  30.723261            94.836   \n",
       "1258   54274900.0           1.52        5.89  31.203734            94.836   \n",
       "1259   54868400.0           1.52        5.89  31.122241            94.836   \n",
       "\n",
       "     Quarterly Net Income  Quarterly Operating Margin      CPI  Interest  \\\n",
       "0                  13.822                      0.2670  251.989    0.0170   \n",
       "1                  13.822                      0.2670  251.989    0.0170   \n",
       "2                  13.822                      0.2670  251.989    0.0190   \n",
       "3                  13.822                      0.2670  251.989    0.0190   \n",
       "4                  13.822                      0.2670  251.989    0.0190   \n",
       "...                   ...                         ...      ...       ...   \n",
       "1255               24.160                      0.2916  304.127    0.0508   \n",
       "1256               24.160                      0.2916  304.127    0.0508   \n",
       "1257               24.160                      0.2916  304.127    0.0508   \n",
       "1258               24.160                      0.2916  304.127    0.0508   \n",
       "1259               24.160                      0.2916  304.127    0.0508   \n",
       "\n",
       "      Flag      PCE    VIX  \n",
       "0      0.0  13900.2  12.34  \n",
       "1      0.0  13900.2  12.94  \n",
       "2      0.0  13900.2  12.12  \n",
       "3      0.0  13900.2  11.98  \n",
       "4      0.0  13900.2  12.31  \n",
       "...    ...      ...    ...  \n",
       "1255   0.0  23096.9  13.94  \n",
       "1256   0.0  23096.9  13.65  \n",
       "1257   0.0  23096.9  13.83  \n",
       "1258   0.0  23096.9  15.01  \n",
       "1259   0.0  23096.9  14.61  \n",
       "\n",
       "[1260 rows x 18 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data(input_date):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    import requests\n",
    "    import datetime\n",
    "    import yfinance as yf\n",
    "    \n",
    "    data= yf.download(\"AAPL\",\"2018-06-12\", input_date)\n",
    "    stock_info_df= pd.DataFrame(data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']])\n",
    "    stock_info_df = stock_info_df.sort_values(by='Date',ascending=True)\n",
    "    stock_info_df.reset_index(drop=False, inplace=True)\n",
    "    stock_info_df['Date'] = pd.to_datetime(stock_info_df['Date'])\n",
    "\n",
    "\n",
    "\n",
    "    # Define the start and end dates\n",
    "    start_date = '2018-01-01'\n",
    "    end_date = input_date\n",
    "\n",
    "    # Create an empty list to store the dates\n",
    "    dates = []\n",
    "\n",
    "    # Iterate over each month\n",
    "    for year in range(pd.to_datetime(start_date).year, pd.to_datetime(end_date).year + 1):\n",
    "        for month in range(1, 13):\n",
    "            # Create the start and end dates for the current month\n",
    "            month_start = f\"{year}-{month:02d}-01\"\n",
    "            month_end = pd.to_datetime(month_start) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "            # Generate a range of dates for the current month\n",
    "            month_dates = pd.date_range(start=month_start, end=month_end, freq='D')\n",
    "\n",
    "            # Append the dates to the list\n",
    "            dates.extend(month_dates)\n",
    "\n",
    "    # Create a DataFrame from the list of dates\n",
    "    all_days_df = pd.DataFrame({'Date': dates})\n",
    "\n",
    "    # replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
    "    url = 'https://www.alphavantage.co/query?function=CPI&interval=monthly&apikey=WIB4EPEUDA7KJFOG'\n",
    "    r = requests.get(url)\n",
    "    data_str = r.text\n",
    "\n",
    "    # Parse the string into a dictionary\n",
    "    data = json.loads(data_str)\n",
    "\n",
    "    df_CPI = pd.DataFrame(data['data'])\n",
    "    df_CPI.rename(columns={'date': 'Date'}, inplace=True)\n",
    "    df_CPI.rename(columns={'value': 'CPI'}, inplace=True)\n",
    "    df_CPI = df_CPI[df_CPI['Date'] >= '2018-05-12']\n",
    "    df_CPI = df_CPI.sort_values(by='Date',ascending=True)\n",
    "    df_CPI.reset_index(drop=True, inplace=True)\n",
    "    df_CPI['Date'] = pd.to_datetime(df_CPI['Date'])\n",
    "\n",
    "    df_CPI_daily = all_days_df.merge(df_CPI, on='Date', how='left')\n",
    "\n",
    "    df_CPI_daily['CPI'] = df_CPI_daily['CPI'].fillna(method='ffill')\n",
    "\n",
    "    df_CPI_daily = df_CPI_daily[df_CPI_daily['Date'] >= '2018-06-12']\n",
    "    df_CPI_daily = df_CPI_daily[df_CPI_daily['Date'] <= input_date]\n",
    "\n",
    "    # replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
    "    url = 'https://www.alphavantage.co/query?function=FEDERAL_FUNDS_RATE&interval=daily&apikey=WIB4EPEUDA7KJFOG'\n",
    "    r = requests.get(url)\n",
    "    data_str = r.text\n",
    "\n",
    "    # Parse the string into a dictionary\n",
    "    data = json.loads(data_str)\n",
    "\n",
    "    df_interest = pd.DataFrame(data['data'])\n",
    "    df_interest.rename(columns={'date': 'Date'}, inplace=True)\n",
    "    df_interest.rename(columns={'value': 'Interest'}, inplace=True)\n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_interest['Date'] = pd.to_datetime(df_interest['Date'])\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df_interest = df_interest[df_interest['Date'] >= '2018-06-12']\n",
    "    df_CPI_daily = df_CPI_daily[df_CPI_daily['Date'] <= input_date]\n",
    "    df_interest = df_interest.sort_values(by='Date',ascending=True)\n",
    "    # Convert 'Interest' column to numeric\n",
    "    df_interest['Interest'] = pd.to_numeric(df_interest['Interest'], errors='coerce')\n",
    "    df_interest['Interest'] = df_interest['Interest'] / 100\n",
    "    df_interest.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Convert the 'Date' column in df_interest to datetime data type\n",
    "    df_interest['Date'] = pd.to_datetime(df_interest['Date'])\n",
    "    # Convert the 'Date' column in df_CPI to datetime data type\n",
    "\n",
    "    # Perform the merge\n",
    "    df_interest_cpi = df_CPI_daily.merge(df_interest, on='Date', how='left')\n",
    "\n",
    "    # Specify the file path of the CSV file\n",
    "    csv_file_path = '/Users/emanuelbayat/code/Fetyukovitch/SMA/raw_data/AAPL_Financials.csv'\n",
    "\n",
    "    # Read the CSV file using pandas\n",
    "    aapl_finances_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    #12th junen 2018\n",
    "    aapl_finances_df = aapl_finances_df.drop(aapl_finances_df[aapl_finances_df.index > 22].index)\n",
    "\n",
    "    aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'].str.replace('$', '')\n",
    "    aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'].str.replace('$', '')\n",
    "    aapl_finances_df['Apple Quarterly EPS'] = aapl_finances_df['Apple Quarterly EPS'].str.replace('$','')\n",
    "    aapl_finances_df['Apple Quarterly Operating Margin'] = aapl_finances_df['Apple Quarterly Operating Margin'].str.replace('%','')\n",
    "    # Convert the column to numeric type\n",
    "    aapl_finances_df['Apple Quarterly Operating Margin'] = pd.to_numeric(aapl_finances_df['Apple Quarterly Operating Margin'], errors='coerce')\n",
    "    aapl_finances_df['Apple Quarterly Operating Margin'] = aapl_finances_df['Apple Quarterly Operating Margin'] / 100\n",
    "\n",
    "    aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'].str.replace(',', '.')\n",
    "    aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'].str.replace(',', '.')\n",
    "    aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'].str.replace(',', '.')\n",
    "    aapl_finances_df=aapl_finances_df.sort_values(by='Dates',ascending=True)\n",
    "    aapl_finances_df.reset_index(drop=True, inplace=True)\n",
    "    aapl_finances_df.rename(columns={'Dates': 'Date'}, inplace=True)\n",
    "\n",
    "    # Assuming the DataFrame is already defined as \"aapl_finances_df\"\n",
    "\n",
    "    # Convert the \"Date\" column to datetime format\n",
    "    aapl_finances_df['Date'] = pd.to_datetime(aapl_finances_df['Date'])\n",
    "\n",
    "    # Add a new column \"Day of Week\" representing the day of the week as a number (0-6)\n",
    "    aapl_finances_df['Day of Week'] = aapl_finances_df['Date'].dt.weekday\n",
    "\n",
    "    # Assuming the DataFrame is already defined as \"aapl_finances_df\"\n",
    "\n",
    "    # Convert the \"Date\" column to datetime format\n",
    "    aapl_finances_df['Date'] = pd.to_datetime(aapl_finances_df['Date'])\n",
    "\n",
    "    # Add a new column \"Day of Week\" representing the day of the week as a number (0-6)\n",
    "    aapl_finances_df['Day of Week'] = aapl_finances_df['Date'].dt.weekday\n",
    "\n",
    "    # Subtract the corresponding value from the day if \"Day of Week\" is greater than 4\n",
    "    aapl_finances_df.loc[aapl_finances_df['Day of Week'] > 4, 'Date'] -= pd.to_timedelta(aapl_finances_df['Day of Week'] - 4, unit='D')\n",
    "\n",
    "    # Update the \"Day of Week\" column to represent the correct day of the week (4 for values > 4)\n",
    "    aapl_finances_df['Day of Week'] = aapl_finances_df['Date'].dt.weekday\n",
    "\n",
    "    aapl_finances_df = aapl_finances_df.drop('Day of Week', axis=1)\n",
    "\n",
    "    df_EPS=pd.DataFrame(aapl_finances_df[['Date', 'Apple Quarterly EPS']])\n",
    "\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_new\n",
    "    stock_info_df['Date'] = pd.to_datetime(stock_info_df['Date'])\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_EPS\n",
    "    df_EPS['Date'] = pd.to_datetime(df_EPS['Date'])\n",
    "\n",
    "    # Merge df_EPS onto df_new using left merge\n",
    "    merged_financial_df = stock_info_df.merge(df_EPS, on='Date', how='left')\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_new\n",
    "    stock_info_df['Date'] = pd.to_datetime(stock_info_df['Date'])\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_EPS\n",
    "    df_EPS['Date'] = pd.to_datetime(df_EPS['Date'])\n",
    "\n",
    "    # Merge df_EPS onto df_new using left merge\n",
    "    merged_financial_df = stock_info_df.merge(df_EPS, on='Date', how='left')\n",
    "\n",
    "    # Find missing date entries\n",
    "    missing_dates = df_EPS[~df_EPS['Date'].isin(merged_financial_df['Date'])]\n",
    "\n",
    "    # Create a new DataFrame with missing dates\n",
    "    df_new = pd.DataFrame(missing_dates)\n",
    "\n",
    "    # Add the entries from df_new to merged_financial_df\n",
    "    merged_financial_df = pd.concat([merged_financial_df, df_new])\n",
    "\n",
    "    # Sort the DataFrame by the \"Date\" column\n",
    "    merged_financial_df = merged_financial_df.sort_values('Date')\n",
    "\n",
    "    # Reset the index\n",
    "    merged_financial_df = merged_financial_df.reset_index(drop=True)\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    merged_financial_df['Apple Quarterly EPS'] = merged_financial_df['Apple Quarterly EPS'].fillna(method='ffill')\n",
    "\n",
    "    merged_financial_df = merged_financial_df.sort_values('Date', ascending=True)\n",
    "    merged_financial_df['Yearly EPS'] = np.nan\n",
    "\n",
    "    start_date = pd.to_datetime('2018-06-12')\n",
    "\n",
    "    merged_financial_df['Apple Quarterly EPS'] = merged_financial_df['Apple Quarterly EPS'].astype(float)\n",
    "\n",
    "    for index, row in merged_financial_df.iterrows():\n",
    "        current_date = row['Date']\n",
    "\n",
    "        if current_date >= start_date:\n",
    "            previous_values = merged_financial_df[(merged_financial_df['Date'] < current_date) & (~merged_financial_df['Apple Quarterly EPS'].isna())]['Apple Quarterly EPS'].unique()\n",
    "            if len(previous_values) >= 3:\n",
    "                previous_values = previous_values[-3:]\n",
    "                yearly_eps = row['Apple Quarterly EPS'] + previous_values.sum()\n",
    "                merged_financial_df.at[index, 'Yearly EPS'] = yearly_eps\n",
    "\n",
    "\n",
    "    # Convert \"Date\" column to datetime format\n",
    "    merged_financial_df['Date'] = pd.to_datetime(merged_financial_df['Date'])\n",
    "\n",
    "    # Calculate the PE ratio\n",
    "    merged_financial_df['PE'] = merged_financial_df['Close'] / merged_financial_df['Yearly EPS']\n",
    "\n",
    "    # Filter data from the date 2018-06-29 onwards\n",
    "    merged_financial_df = merged_financial_df[merged_financial_df['Date'] >= '2018-06-12']\n",
    "\n",
    "    merged_financial_df = merged_financial_df.copy()\n",
    "    #merged_financial_df.rename(columns={'Apple Quarterly EPS': 'Quarterly EPS'}, inplace=True)\n",
    "\n",
    "    aapl_finances_df = all_days_df.merge(aapl_finances_df, on='Date', how='left')\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Revenue\\n(Millions of US $)'].fillna(method='ffill')\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'] = aapl_finances_df['Apple Quarterly Net Income\\n(Millions of US $)'].fillna(method='ffill')\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    aapl_finances_df['Apple Quarterly Operating Margin'] = aapl_finances_df['Apple Quarterly Operating Margin'].fillna(method='ffill')\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    aapl_finances_df['Apple Quarterly EPS'] = aapl_finances_df['Apple Quarterly EPS'].fillna(method='ffill')\n",
    "\n",
    "    # Filter the DataFrame to keep only the entries on or after '2018-06-12'\n",
    "    aapl_finances_df = aapl_finances_df[aapl_finances_df['Date'] >= '2018-06-12']\n",
    "    aapl_finances_df = aapl_finances_df[aapl_finances_df['Date'] <= input_date]\n",
    "    # Reset the index of the DataFrame\n",
    "    aapl_finances_df = aapl_finances_df.reset_index(drop=True)\n",
    "\n",
    "    df_final = merged_financial_df.merge(aapl_finances_df, on='Date', how='left')\n",
    "\n",
    "    df_final = df_final.merge(df_interest_cpi, on='Date', how='left')\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Sorted dates without duplicates\n",
    "    sorted_dates = [\n",
    "        '2018-07-12', '2018-09-21', '2018-10-26', '2018-10-30', '2018-11-07',\n",
    "        '2019-03-18', '2019-03-19', '2019-03-20', '2019-03-25', '2019-05-21',\n",
    "        '2019-05-28', '2019-06-03', '2019-06-04', '2019-06-05', '2019-06-06',\n",
    "        '2019-06-07', '2019-07-09', '2019-08-20', '2019-09-10', '2019-09-20',\n",
    "        '2019-09-25', '2020-03-18', '2020-04-24', '2020-05-04', '2020-06-22',\n",
    "        '2020-06-23', '2020-06-24', '2020-06-25', '2020-06-26', '2020-08-04',\n",
    "        '2020-09-15', '2020-09-18', '2020-10-13', '2020-10-23', '2020-11-10',\n",
    "        '2020-11-13', '2020-11-16', '2020-11-17', '2020-12-15', '2021-04-20',\n",
    "        '2021-04-30', '2021-05-21', '2021-07-13', '2021-09-14', '2021-09-24',\n",
    "        '2021-10-08', '2021-10-18', '2021-10-26', '2021-11-01', '2022-03-08',\n",
    "        '2022-03-18', '2022-06-06', '2022-06-07', '2022-06-08', '2022-06-09',\n",
    "        '2022-06-10', '2022-06-24', '2022-07-15', '2022-09-07', '2022-09-16',\n",
    "        '2022-09-23', '2022-10-07', '2022-10-26', '2022-11-04', '2023-01-24',\n",
    "        '2023-02-03', '2023-06-05' , '2023-06-06'\n",
    "    ]\n",
    "\n",
    "    # Create DataFrame with dates and a column with all entries as \"1\"\n",
    "    flag_df = pd.DataFrame({'Date': sorted_dates})\n",
    "    flag_df['Flag'] = 1\n",
    "\n",
    "    # Display the DataFrame\n",
    "    event_dates = flag_df[\"Date\"].to_list()\n",
    "\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_EPS\n",
    "    flag_df['Date'] = pd.to_datetime(flag_df['Date'])\n",
    "\n",
    "    # Merge df_EPS onto df_new using left merge\n",
    "    df_final = df_final.merge(flag_df, on='Date', how='left')\n",
    "    df_final['Flag'] = df_final['Flag'].fillna(0)\n",
    "\n",
    "\n",
    "    # Replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
    "    url = 'https://api.stlouisfed.org/fred/series/observations'\n",
    "    params = {\n",
    "        'series_id': 'PCE',\n",
    "        'api_key': '55f5943ac55f44a9be924f4751900af3',\n",
    "        'file_type': 'json',\n",
    "        'observation_start': '2018-06-12',\n",
    "        'observation_end': input_date\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params)\n",
    "    data = r.json()\n",
    "    # Extract the 'observations' list from the JSON data\n",
    "    observations = data['observations']\n",
    "    # Create a pandas DataFrame with only 'date' and 'value' columns\n",
    "    PCE_monthly_df = pd.DataFrame(observations, columns=['date', 'value'])\n",
    "    # Convert 'value' column to float\n",
    "    PCE_monthly_df['value'] = PCE_monthly_df['value'].astype(float)\n",
    "    PCE_monthly_df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "    PCE_monthly_df.rename(columns={'value': 'PCE'}, inplace=True)\n",
    "\n",
    "    # Calculate the slope using linear regression\n",
    "    values = PCE_monthly_df['PCE'].values\n",
    "    time = np.arange(len(values))\n",
    "    slope, _ = np.polyfit(time, values, 1)\n",
    "\n",
    "    # Get the last date in the DataFrame\n",
    "    last_date = datetime.datetime.strptime(PCE_monthly_df['Date'].iloc[-1], '%Y-%m-%d')\n",
    "\n",
    "    # Generate the next two months' dates (first day of the month)\n",
    "    next_date_1 = datetime.datetime(last_date.year, last_date.month + 1, 1)\n",
    "    next_date_2 = datetime.datetime(last_date.year, last_date.month + 2, 1)\n",
    "\n",
    "    # Create a DataFrame with the extrapolated dates\n",
    "    extrapolated_dates = pd.DataFrame({'Date': [next_date_1, next_date_2]})\n",
    "\n",
    "    # Extrapolate the next two months' values based on the calculated slope\n",
    "    extrapolated_values = values[-1] + slope * (len(values) + np.arange(2))\n",
    "\n",
    "    # Add the extrapolated values to the DataFrame\n",
    "    extrapolated_dates['PCE'] = extrapolated_values\n",
    "\n",
    "    # Concatenate the extrapolated dates DataFrame with the original DataFrame\n",
    "    extrapolated_df = pd.concat([PCE_monthly_df, extrapolated_dates])\n",
    "\n",
    "    extrapolated_df['Date'] = pd.to_datetime(extrapolated_df['Date']).dt.strftime('%Y-%m-%d')\n",
    "    # Round the values in the 'PCE' column to one decimal point\n",
    "    extrapolated_df['PCE'] = extrapolated_df['PCE'].round(1)\n",
    "\n",
    "    extrapolated_df = extrapolated_df.reset_index(drop=True)\n",
    "\n",
    "    # Convert 'Date' column to datetime format\n",
    "    extrapolated_df['Date'] = pd.to_datetime(extrapolated_df['Date'])\n",
    "\n",
    "    # Create a new DataFrame with a 'Date' column containing all dates from 2018-06-01 to 2023-06-30\n",
    "    date_range = pd.date_range(start='2018-06-01', end=input_date, freq='D')\n",
    "    new_2_df = pd.DataFrame({'Date': date_range})\n",
    "\n",
    "    # Convert 'Date' column to datetime format\n",
    "    new_2_df['Date'] = pd.to_datetime(new_2_df['Date'])\n",
    "\n",
    "    # Merge the new DataFrame with 'extrapolated_df' based on the 'Date' column\n",
    "    PCE_daily_df = pd.merge(new_2_df, extrapolated_df, on='Date', how='left')\n",
    "\n",
    "    # Forward fill the 'PCE' column to fill missing values with the corresponding month's constant value\n",
    "    PCE_daily_df['PCE'] = PCE_daily_df['PCE'].fillna(method='ffill')\n",
    "\n",
    "\n",
    "    # Convert \"Date\" column to datetime in df_new\n",
    "    PCE_daily_df['Date'] = pd.to_datetime(PCE_daily_df['Date'])\n",
    "\n",
    "    # Merge df_EPS onto df_new using left merge\n",
    "    df_final = df_final.merge(PCE_daily_df, on='Date', how='left')\n",
    "\n",
    "\n",
    "    # Replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
    "    url = 'https://api.stlouisfed.org/fred/series/observations'\n",
    "    params = {\n",
    "        'series_id': 'VIXCLS',\n",
    "        'api_key': '55f5943ac55f44a9be924f4751900af3',\n",
    "        'file_type': 'json',\n",
    "        'observation_start': '2018-06-12',\n",
    "        'observation_end': input_date\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params)\n",
    "    data = r.json()\n",
    "\n",
    "    # Extract the 'observations' list from the JSON data\n",
    "    observations = data['observations']\n",
    "\n",
    "    # Create a pandas DataFrame with only 'date' and 'value' columns\n",
    "    vix_daily_df = pd.DataFrame(observations, columns=['date', 'value'])\n",
    "\n",
    "    vix_daily_df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "    vix_daily_df.rename(columns={'value': 'VIX'}, inplace=True)\n",
    "    \n",
    "    # Convert \"Date\" column to datetime in df_new\n",
    "    vix_daily_df['Date'] = pd.to_datetime(vix_daily_df['Date'])\n",
    "\n",
    "    # Merge df_EPS onto df_new using left merge\n",
    "    df_final = df_final.merge(vix_daily_df, on='Date', how='left')\n",
    "\n",
    "    symbol = \"^VIX\"\n",
    "    data = yf.Ticker(symbol)\n",
    "    historical_data = data.history(period=\"5d\")  # Fetching data for a longer period (5 days)\n",
    "    previous_close = historical_data[\"Close\"].iloc[-2]  # Using iloc to retrieve the second-to-last value\n",
    "    df_final.loc[df_final.index[-1], \"VIX\"] = previous_close\n",
    "\n",
    "    df_final['Interest'] = df_final['Interest'].fillna(method='ffill')\n",
    "\n",
    "    df_final = df_final.drop('Apple Quarterly EPS_y', axis=1)\n",
    "    df_final.rename(columns={'Apple Quarterly EPS_x': 'Quarterly EPS'}, inplace=True)\n",
    "    df_final.rename(columns={'Apple Quarterly Revenue\\n(Millions of US $)': 'Quarterly Revenue'}, inplace=True)\n",
    "    df_final.rename(columns={'Apple Quarterly Net Income\\n(Millions of US $)': 'Quarterly Net Income'}, inplace=True)\n",
    "    df_final.rename(columns={'Apple Quarterly Operating Margin': 'Quarterly Operating Margin'}, inplace=True)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "input_date = '2023-06-14'  # Replace with your desired date\n",
    "output_data = get_data(input_date)\n",
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90436305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
