{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2847af65-2b10-4b3e-8e58-9b89811185ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 15:08:27.330240: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 15:08:27.611374: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-07 15:08:27.611400: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-07 15:08:27.647193: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-07 15:08:28.508387: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-07 15:08:28.508447: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-07 15:08:28.508452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02722ea-9991-4d99-92cd-346c3acd7cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>content</th>\n",
       "      <th>created_at</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPG</td>\n",
       "      <td>Your Morning Coffee: 10/05/2018 #EARNINGS\\n\\n ...</td>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPG</td>\n",
       "      <td>Tendiegenerator.java     import java.util.Arra...</td>\n",
       "      <td>2020-10-04</td>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IPG</td>\n",
       "      <td>J.P. Morgan Early Look at the Market – Mon 10....</td>\n",
       "      <td>2017-10-16</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol                                            content  created_at  score\n",
       "0    IPG  Your Morning Coffee: 10/05/2018 #EARNINGS\\n\\n ...  2018-10-05  243.0\n",
       "1    IPG  Tendiegenerator.java     import java.util.Arra...  2020-10-04  185.0\n",
       "2    IPG  J.P. Morgan Early Look at the Market – Mon 10....  2017-10-16  145.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../raw_data/reddit_all_symbols.csv',)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "671f34ef-8105-497a-87fc-5d62497aa9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "cleanr = re.compile('<.*?>')\n",
    "data['content'] = data['content'].apply(lambda x: re.sub('[0-9]+', '', re.sub(r'http\\S+', '', re.sub(cleanr, '', x))))\n",
    "#rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "#rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "data['content'] = data['content'].apply(lambda x : x.lower().replace('\\n', ' ').strip())\n",
    "#data['content'] = data['content'].apply(lambda x: re.sub('[0-9]+', '', x))\n",
    "X = data.loc[:20, ['symbol', 'content']]\n",
    "X['content'] = X['content'].apply(lambda x: x[0:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf204108-cce2-42fa-95f3-db9332012bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPG</td>\n",
       "      <td>your morning coffee: // #earnings    costco (c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPG</td>\n",
       "      <td>tendiegenerator.java     import java.util.arra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IPG</td>\n",
       "      <td>j.p. morgan early look at the market – mon .. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IPG</td>\n",
       "      <td>wall street week ahead for the trading week be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IPG</td>\n",
       "      <td>your morning coffee [wednesday ..] #earnings  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212310</th>\n",
       "      <td>FIVE</td>\n",
       "      <td>explain to me like i'm five. is hmny really to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212311</th>\n",
       "      <td>FIVE</td>\n",
       "      <td>after holding for five years, selling $fb toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212312</th>\n",
       "      <td>FIVE</td>\n",
       "      <td>oracle just somehow jumped % in five minutes? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212313</th>\n",
       "      <td>FIVE</td>\n",
       "      <td>five of nine seems fine for usa mines &amp;#xb;  [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212314</th>\n",
       "      <td>FIVE</td>\n",
       "      <td>all five below locations | discount store, nov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212315 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       symbol                                            content\n",
       "0         IPG  your morning coffee: // #earnings    costco (c...\n",
       "1         IPG  tendiegenerator.java     import java.util.arra...\n",
       "2         IPG  j.p. morgan early look at the market – mon .. ...\n",
       "3         IPG  wall street week ahead for the trading week be...\n",
       "4         IPG  your morning coffee [wednesday ..] #earnings  ...\n",
       "...       ...                                                ...\n",
       "212310   FIVE  explain to me like i'm five. is hmny really to...\n",
       "212311   FIVE  after holding for five years, selling $fb toda...\n",
       "212312   FIVE  oracle just somehow jumped % in five minutes? ...\n",
       "212313   FIVE  five of nine seems fine for usa mines &#xb;  [...\n",
       "212314   FIVE  all five below locations | discount store, nov...\n",
       "\n",
       "[212315 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.loc[:, ['symbol', 'content']]\n",
    "X['content'] = X['content'].apply(lambda x: x[0:512])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81f05909-be90-4bf1-abc3-4090d8c50fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# The tokenization learns a dictionary that maps a token (integer) to each word\n",
    "# It can be done only on the train set - we are not supposed to know the test set!\n",
    "# This tokenization also lowercases your words, apply some filters, and so on - you can check the doc if you want\n",
    "text = tokenizer.fit_on_texts(X['content'])\n",
    "    \n",
    "# We apply the tokenization to the train and test set\n",
    "X_token = tokenizer.texts_to_sequences(X['content'])\n",
    "#X_test_token = tokenizer.texts_to_sequences(X_test)\n",
    "tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb82925e-c759-439d-9771-b307300ff17d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/lico/.local/lib/python3.10/site-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/lico/.local/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lico/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lico/.local/lib/python3.10/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lico/.local/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: requests in /home/lico/.local/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/lico/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /home/lico/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/lico/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/lico/.local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lico/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lico/.local/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cef624e1-bcf9-487d-90a8-ef93428dbcfd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [72], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#print(sentences)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m results\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1100\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1097\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1098\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1099\u001b[0m     )\n\u001b[0;32m-> 1100\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1025\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1024\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1025\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:182\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:462\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 462\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales.\",  \n",
    "             \"Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least EUR 4,000.\", \n",
    "             \"Raute reported a loss per share of EUR 0.86 for the first half of 2009 , against EPS of EUR 0.74 in the corresponding period of 2008.\", \n",
    "             ]\n",
    "sentences = list(X['content'])\n",
    "#print(sentences)\n",
    "results = nlp(sentences)\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e649b1a5-266f-4da4-a75f-7d5ea5677105",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.9915090799331665},\n",
       " {'label': 'neutral', 'score': 0.9991363883018494},\n",
       " {'label': 'neutral', 'score': 0.9667267799377441},\n",
       " {'label': 'positive', 'score': 0.9967592358589172},\n",
       " {'label': 'negative', 'score': 0.9984101057052612},\n",
       " {'label': 'neutral', 'score': 0.9992467164993286},\n",
       " {'label': 'neutral', 'score': 0.997963547706604},\n",
       " {'label': 'neutral', 'score': 0.9980545043945312},\n",
       " {'label': 'neutral', 'score': 0.9985528588294983},\n",
       " {'label': 'neutral', 'score': 0.9962025284767151},\n",
       " {'label': 'neutral', 'score': 0.9997654557228088},\n",
       " {'label': 'positive', 'score': 0.9996376037597656},\n",
       " {'label': 'neutral', 'score': 0.9941148161888123},\n",
       " {'label': 'neutral', 'score': 0.9996575117111206},\n",
       " {'label': 'neutral', 'score': 0.999605119228363},\n",
       " {'label': 'neutral', 'score': 0.9995415210723877},\n",
       " {'label': 'neutral', 'score': 0.9959171414375305},\n",
       " {'label': 'neutral', 'score': 0.9944250583648682},\n",
       " {'label': 'neutral', 'score': 0.9967824220657349},\n",
       " {'label': 'neutral', 'score': 0.9997510313987732},\n",
       " {'label': 'neutral', 'score': 0.9986201524734497},\n",
       " {'label': 'neutral', 'score': 0.9706199169158936},\n",
       " {'label': 'neutral', 'score': 0.9997734427452087},\n",
       " {'label': 'neutral', 'score': 0.9985887408256531},\n",
       " {'label': 'neutral', 'score': 0.9997802376747131},\n",
       " {'label': 'neutral', 'score': 0.9988835453987122},\n",
       " {'label': 'neutral', 'score': 0.9991046786308289},\n",
       " {'label': 'negative', 'score': 0.993371844291687},\n",
       " {'label': 'neutral', 'score': 0.9996832609176636},\n",
       " {'label': 'positive', 'score': 0.9988419413566589},\n",
       " {'label': 'neutral', 'score': 0.9997275471687317},\n",
       " {'label': 'neutral', 'score': 0.9895893931388855},\n",
       " {'label': 'neutral', 'score': 0.9964296221733093},\n",
       " {'label': 'neutral', 'score': 0.8739298582077026},\n",
       " {'label': 'neutral', 'score': 0.98653644323349},\n",
       " {'label': 'neutral', 'score': 0.9991114735603333},\n",
       " {'label': 'neutral', 'score': 0.956104576587677},\n",
       " {'label': 'neutral', 'score': 0.9993630051612854},\n",
       " {'label': 'neutral', 'score': 0.9997238516807556},\n",
       " {'label': 'positive', 'score': 0.9980660080909729},\n",
       " {'label': 'neutral', 'score': 0.998390793800354},\n",
       " {'label': 'neutral', 'score': 0.9986749291419983},\n",
       " {'label': 'neutral', 'score': 0.9995588660240173},\n",
       " {'label': 'neutral', 'score': 0.9982593655586243},\n",
       " {'label': 'neutral', 'score': 0.9996656179428101},\n",
       " {'label': 'neutral', 'score': 0.9996395111083984},\n",
       " {'label': 'neutral', 'score': 0.9993902444839478},\n",
       " {'label': 'neutral', 'score': 0.9993093013763428},\n",
       " {'label': 'neutral', 'score': 0.9901846647262573},\n",
       " {'label': 'neutral', 'score': 0.9992197751998901},\n",
       " {'label': 'neutral', 'score': 0.9997071623802185},\n",
       " {'label': 'neutral', 'score': 0.9996824264526367},\n",
       " {'label': 'neutral', 'score': 0.999333918094635},\n",
       " {'label': 'positive', 'score': 0.9996650218963623},\n",
       " {'label': 'neutral', 'score': 0.9995760321617126},\n",
       " {'label': 'neutral', 'score': 0.9994774460792542},\n",
       " {'label': 'neutral', 'score': 0.998611569404602},\n",
       " {'label': 'neutral', 'score': 0.9793669581413269},\n",
       " {'label': 'neutral', 'score': 0.9993613362312317},\n",
       " {'label': 'neutral', 'score': 0.9962453246116638},\n",
       " {'label': 'neutral', 'score': 0.9992567896842957},\n",
       " {'label': 'neutral', 'score': 0.9606154561042786},\n",
       " {'label': 'neutral', 'score': 0.998731791973114},\n",
       " {'label': 'neutral', 'score': 0.9997557997703552},\n",
       " {'label': 'neutral', 'score': 0.9997590184211731},\n",
       " {'label': 'neutral', 'score': 0.9987764954566956},\n",
       " {'label': 'neutral', 'score': 0.5154924988746643},\n",
       " {'label': 'neutral', 'score': 0.9977211356163025},\n",
       " {'label': 'neutral', 'score': 0.9995061159133911},\n",
       " {'label': 'neutral', 'score': 0.9965910911560059},\n",
       " {'label': 'neutral', 'score': 0.9995415210723877},\n",
       " {'label': 'neutral', 'score': 0.9986899495124817},\n",
       " {'label': 'neutral', 'score': 0.9957308173179626},\n",
       " {'label': 'neutral', 'score': 0.9992302656173706},\n",
       " {'label': 'neutral', 'score': 0.9743354320526123},\n",
       " {'label': 'neutral', 'score': 0.9747176170349121},\n",
       " {'label': 'neutral', 'score': 0.9989243149757385},\n",
       " {'label': 'positive', 'score': 0.9992550015449524},\n",
       " {'label': 'neutral', 'score': 0.9994320273399353},\n",
       " {'label': 'neutral', 'score': 0.9993728995323181},\n",
       " {'label': 'neutral', 'score': 0.9981743097305298},\n",
       " {'label': 'neutral', 'score': 0.9997636675834656},\n",
       " {'label': 'neutral', 'score': 0.9984922409057617},\n",
       " {'label': 'neutral', 'score': 0.9995249509811401},\n",
       " {'label': 'neutral', 'score': 0.9960575103759766},\n",
       " {'label': 'neutral', 'score': 0.9969474673271179},\n",
       " {'label': 'neutral', 'score': 0.9991403818130493},\n",
       " {'label': 'neutral', 'score': 0.9989997744560242},\n",
       " {'label': 'neutral', 'score': 0.9997063279151917},\n",
       " {'label': 'neutral', 'score': 0.9972896575927734},\n",
       " {'label': 'neutral', 'score': 0.9992623925209045},\n",
       " {'label': 'positive', 'score': 0.6998294591903687},\n",
       " {'label': 'neutral', 'score': 0.9994844198226929},\n",
       " {'label': 'neutral', 'score': 0.9996689558029175},\n",
       " {'label': 'neutral', 'score': 0.9996881484985352},\n",
       " {'label': 'neutral', 'score': 0.9994219541549683},\n",
       " {'label': 'positive', 'score': 0.9678981900215149},\n",
       " {'label': 'neutral', 'score': 0.9610177874565125},\n",
       " {'label': 'neutral', 'score': 0.9995005130767822},\n",
       " {'label': 'neutral', 'score': 0.9996763467788696},\n",
       " {'label': 'neutral', 'score': 0.7340688109397888},\n",
       " {'label': 'neutral', 'score': 0.9985817670822144},\n",
       " {'label': 'neutral', 'score': 0.9994763731956482},\n",
       " {'label': 'neutral', 'score': 0.9421652555465698},\n",
       " {'label': 'neutral', 'score': 0.9983709454536438},\n",
       " {'label': 'neutral', 'score': 0.9982810020446777},\n",
       " {'label': 'neutral', 'score': 0.9958256483078003},\n",
       " {'label': 'neutral', 'score': 0.907490074634552},\n",
       " {'label': 'positive', 'score': 0.9794948101043701},\n",
       " {'label': 'neutral', 'score': 0.9992486834526062},\n",
       " {'label': 'neutral', 'score': 0.9895768165588379},\n",
       " {'label': 'neutral', 'score': 0.9996452331542969},\n",
       " {'label': 'positive', 'score': 0.9979215264320374},\n",
       " {'label': 'neutral', 'score': 0.9932215213775635},\n",
       " {'label': 'neutral', 'score': 0.9987952709197998},\n",
       " {'label': 'positive', 'score': 0.9698458313941956},\n",
       " {'label': 'neutral', 'score': 0.9987890124320984},\n",
       " {'label': 'neutral', 'score': 0.9961550831794739},\n",
       " {'label': 'neutral', 'score': 0.9995754361152649},\n",
       " {'label': 'neutral', 'score': 0.9981809854507446},\n",
       " {'label': 'neutral', 'score': 0.9993517994880676},\n",
       " {'label': 'neutral', 'score': 0.9989712238311768},\n",
       " {'label': 'positive', 'score': 0.9996281862258911},\n",
       " {'label': 'neutral', 'score': 0.999647855758667},\n",
       " {'label': 'neutral', 'score': 0.9640325307846069},\n",
       " {'label': 'neutral', 'score': 0.9942002296447754},\n",
       " {'label': 'neutral', 'score': 0.9996780157089233},\n",
       " {'label': 'neutral', 'score': 0.9981217980384827},\n",
       " {'label': 'positive', 'score': 0.5425409078598022},\n",
       " {'label': 'neutral', 'score': 0.9714177250862122},\n",
       " {'label': 'neutral', 'score': 0.8963971138000488},\n",
       " {'label': 'neutral', 'score': 0.999043881893158},\n",
       " {'label': 'neutral', 'score': 0.9678318500518799},\n",
       " {'label': 'positive', 'score': 0.9937756061553955},\n",
       " {'label': 'neutral', 'score': 0.8760202527046204},\n",
       " {'label': 'neutral', 'score': 0.9994100332260132},\n",
       " {'label': 'neutral', 'score': 0.998751163482666},\n",
       " {'label': 'neutral', 'score': 0.9986310601234436},\n",
       " {'label': 'neutral', 'score': 0.9815258979797363},\n",
       " {'label': 'positive', 'score': 0.999451220035553},\n",
       " {'label': 'negative', 'score': 0.9949918389320374},\n",
       " {'label': 'neutral', 'score': 0.9990235567092896},\n",
       " {'label': 'positive', 'score': 0.8817859888076782},\n",
       " {'label': 'neutral', 'score': 0.9995794892311096},\n",
       " {'label': 'neutral', 'score': 0.9206743240356445},\n",
       " {'label': 'neutral', 'score': 0.9691722989082336},\n",
       " {'label': 'neutral', 'score': 0.999529242515564},\n",
       " {'label': 'neutral', 'score': 0.9929755926132202},\n",
       " {'label': 'neutral', 'score': 0.9945282340049744},\n",
       " {'label': 'neutral', 'score': 0.9997374415397644},\n",
       " {'label': 'neutral', 'score': 0.9986181259155273},\n",
       " {'label': 'neutral', 'score': 0.9984539747238159},\n",
       " {'label': 'neutral', 'score': 0.9992467164993286},\n",
       " {'label': 'neutral', 'score': 0.9608389139175415},\n",
       " {'label': 'neutral', 'score': 0.9957377910614014},\n",
       " {'label': 'neutral', 'score': 0.9008522033691406},\n",
       " {'label': 'neutral', 'score': 0.9893875122070312},\n",
       " {'label': 'neutral', 'score': 0.9991890788078308},\n",
       " {'label': 'neutral', 'score': 0.9997935891151428},\n",
       " {'label': 'neutral', 'score': 0.8607362508773804},\n",
       " {'label': 'neutral', 'score': 0.9995105266571045},\n",
       " {'label': 'negative', 'score': 0.5036478042602539},\n",
       " {'label': 'neutral', 'score': 0.9993170499801636},\n",
       " {'label': 'neutral', 'score': 0.9994168281555176},\n",
       " {'label': 'neutral', 'score': 0.9994812607765198},\n",
       " {'label': 'neutral', 'score': 0.9388023018836975},\n",
       " {'label': 'neutral', 'score': 0.999508261680603},\n",
       " {'label': 'neutral', 'score': 0.9972820281982422},\n",
       " {'label': 'positive', 'score': 0.9998143315315247},\n",
       " {'label': 'neutral', 'score': 0.9997403025627136},\n",
       " {'label': 'neutral', 'score': 0.7882332801818848},\n",
       " {'label': 'neutral', 'score': 0.9627364873886108},\n",
       " {'label': 'neutral', 'score': 0.9417058229446411},\n",
       " {'label': 'positive', 'score': 0.9859189987182617},\n",
       " {'label': 'neutral', 'score': 0.9942294359207153},\n",
       " {'label': 'positive', 'score': 0.9989689588546753},\n",
       " {'label': 'neutral', 'score': 0.9995514750480652},\n",
       " {'label': 'neutral', 'score': 0.9997103810310364},\n",
       " {'label': 'neutral', 'score': 0.9982613921165466},\n",
       " {'label': 'neutral', 'score': 0.9991014003753662},\n",
       " {'label': 'neutral', 'score': 0.9873993396759033},\n",
       " {'label': 'neutral', 'score': 0.983579695224762},\n",
       " {'label': 'neutral', 'score': 0.9983377456665039},\n",
       " {'label': 'neutral', 'score': 0.9969151020050049},\n",
       " {'label': 'neutral', 'score': 0.9993577599525452},\n",
       " {'label': 'neutral', 'score': 0.9994248151779175},\n",
       " {'label': 'neutral', 'score': 0.9995453953742981},\n",
       " {'label': 'neutral', 'score': 0.7633044719696045},\n",
       " {'label': 'neutral', 'score': 0.99920254945755},\n",
       " {'label': 'neutral', 'score': 0.9996718168258667},\n",
       " {'label': 'neutral', 'score': 0.9873033165931702},\n",
       " {'label': 'neutral', 'score': 0.9852727055549622},\n",
       " {'label': 'neutral', 'score': 0.7037194967269897},\n",
       " {'label': 'neutral', 'score': 0.9996315240859985},\n",
       " {'label': 'neutral', 'score': 0.9913827776908875},\n",
       " {'label': 'neutral', 'score': 0.8973459601402283},\n",
       " {'label': 'neutral', 'score': 0.9643664360046387},\n",
       " {'label': 'neutral', 'score': 0.9996234178543091},\n",
       " {'label': 'positive', 'score': 0.9996699094772339},\n",
       " {'label': 'neutral', 'score': 0.9692457914352417},\n",
       " {'label': 'neutral', 'score': 0.99974125623703},\n",
       " {'label': 'neutral', 'score': 0.9994719624519348},\n",
       " {'label': 'neutral', 'score': 0.9986703395843506},\n",
       " {'label': 'neutral', 'score': 0.998721182346344},\n",
       " {'label': 'neutral', 'score': 0.9885237812995911},\n",
       " {'label': 'neutral', 'score': 0.9997696280479431},\n",
       " {'label': 'neutral', 'score': 0.591999351978302},\n",
       " {'label': 'neutral', 'score': 0.9640346169471741},\n",
       " {'label': 'neutral', 'score': 0.996517539024353},\n",
       " {'label': 'neutral', 'score': 0.9986588954925537},\n",
       " {'label': 'neutral', 'score': 0.9684098362922668},\n",
       " {'label': 'neutral', 'score': 0.9961150884628296},\n",
       " {'label': 'neutral', 'score': 0.5387663841247559},\n",
       " {'label': 'neutral', 'score': 0.9984190464019775},\n",
       " {'label': 'positive', 'score': 0.5808879137039185},\n",
       " {'label': 'neutral', 'score': 0.9937912821769714},\n",
       " {'label': 'neutral', 'score': 0.8196913599967957},\n",
       " {'label': 'neutral', 'score': 0.9979478716850281},\n",
       " {'label': 'negative', 'score': 0.992745041847229},\n",
       " {'label': 'neutral', 'score': 0.9972680807113647},\n",
       " {'label': 'neutral', 'score': 0.9992289543151855},\n",
       " {'label': 'neutral', 'score': 0.9995478987693787},\n",
       " {'label': 'neutral', 'score': 0.9905446767807007},\n",
       " {'label': 'positive', 'score': 0.9908877611160278},\n",
       " {'label': 'neutral', 'score': 0.9997344613075256},\n",
       " {'label': 'neutral', 'score': 0.9980648159980774},\n",
       " {'label': 'neutral', 'score': 0.9997225403785706},\n",
       " {'label': 'neutral', 'score': 0.9997509121894836},\n",
       " {'label': 'positive', 'score': 0.8966963887214661},\n",
       " {'label': 'neutral', 'score': 0.9721081256866455},\n",
       " {'label': 'neutral', 'score': 0.991790771484375},\n",
       " {'label': 'neutral', 'score': 0.9916187524795532}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Appl\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "X = pd.read_csv('../raw_data/industries/AAPL.csv')\n",
    "X = X.loc[:, ['symbol', 'content', 'created_at', 'score']]\n",
    "X['content'] = X['content'].apply(lambda x: x[0:512])\n",
    "X\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales.\",  \n",
    "             \"Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least EUR 4,000.\", \n",
    "             \"Raute reported a loss per share of EUR 0.86 for the first half of 2009 , against EPS of EUR 0.74 in the corresponding period of 2008.\", \n",
    "             ]\n",
    "sentences = list(X['content'])\n",
    "#print(sentences)\n",
    "results = nlp(sentences)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e275d4b-74c0-4e8a-8a35-701d83e75990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>content</th>\n",
       "      <th>created_at</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Bears feasting on AAPL next week confirmed</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>21052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Someone purchased $1.3 BILLION worth of $AAPL ...</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>20775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple started WWIII so that phones would be de...</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>15905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>AAPL's innovative product strategy</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>11882</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Chinese Foxconn workers facing off against pol...</td>\n",
       "      <td>2022-11-24</td>\n",
       "      <td>10471</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>$AAPL put the rest of my tiny portfolio on 152...</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>How do you feel about aapl next week? Man, thi...</td>\n",
       "      <td>2023-04-29</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Talking about AAPL positions! Holding 11/20 12...</td>\n",
       "      <td>2020-09-08</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>How likely will AAPL hit 470 by Friday? Bought...</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>The last hour of AMZN and AAPL today The last ...</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    symbol                                            content  created_at  \\\n",
       "0     AAPL        Bears feasting on AAPL next week confirmed   2020-09-26   \n",
       "1     AAPL  Someone purchased $1.3 BILLION worth of $AAPL ...  2022-12-05   \n",
       "2     AAPL  Apple started WWIII so that phones would be de...  2020-01-04   \n",
       "3     AAPL                AAPL's innovative product strategy   2019-01-03   \n",
       "4     AAPL  Chinese Foxconn workers facing off against pol...  2022-11-24   \n",
       "..     ...                                                ...         ...   \n",
       "227   AAPL  $AAPL put the rest of my tiny portfolio on 152...  2022-09-30   \n",
       "228   AAPL  How do you feel about aapl next week? Man, thi...  2023-04-29   \n",
       "229   AAPL  Talking about AAPL positions! Holding 11/20 12...  2020-09-08   \n",
       "230   AAPL  How likely will AAPL hit 470 by Friday? Bought...  2020-08-19   \n",
       "231   AAPL  The last hour of AMZN and AAPL today The last ...  2020-10-17   \n",
       "\n",
       "     score  sentiment  \n",
       "0    21052          0  \n",
       "1    20775          0  \n",
       "2    15905          0  \n",
       "3    11882          1  \n",
       "4    10471         -1  \n",
       "..     ...        ...  \n",
       "227     19          0  \n",
       "228     17          1  \n",
       "229     13          0  \n",
       "230     11          0  \n",
       "231      5          0  \n",
       "\n",
       "[232 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = {\n",
    "    'negative': -1,\n",
    "    'neutral': 0,\n",
    "    'positive': 1,\n",
    "}\n",
    "d = pd.DataFrame(results)\n",
    "X.loc[:, 'sentiment'] = d['label'].apply(lambda x: sentiment[x])\n",
    "X[['symbol', 'score', 'created_at', 'sentiment']]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e46b8e99-68c5-49e7-8700-5ecdcaa66eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('../raw_data/reddit_sentimental_AAPL.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
