{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2847af65-2b10-4b3e-8e58-9b89811185ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 10:13:49.083991: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-09 10:13:49.492372: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-09 10:13:49.492390: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-09 10:13:49.531932: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-09 10:13:50.438455: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-09 10:13:50.438613: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-09 10:13:50.438619: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02722ea-9991-4d99-92cd-346c3acd7cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>content</th>\n",
       "      <th>created_at</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPG</td>\n",
       "      <td>Your Morning Coffee: 10/05/2018 #EARNINGS\\n\\n ...</td>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPG</td>\n",
       "      <td>Tendiegenerator.java     import java.util.Arra...</td>\n",
       "      <td>2020-10-04</td>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IPG</td>\n",
       "      <td>J.P. Morgan Early Look at the Market – Mon 10....</td>\n",
       "      <td>2017-10-16</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol                                            content  created_at  score\n",
       "0    IPG  Your Morning Coffee: 10/05/2018 #EARNINGS\\n\\n ...  2018-10-05  243.0\n",
       "1    IPG  Tendiegenerator.java     import java.util.Arra...  2020-10-04  185.0\n",
       "2    IPG  J.P. Morgan Early Look at the Market – Mon 10....  2017-10-16  145.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../raw_data/reddit_all_symbols.csv',)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671f34ef-8105-497a-87fc-5d62497aa9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#def clean_data(text):\n",
    "#    cleanr = re.compile('<.*?>')\n",
    "#    text = text.apply(lambda x: re.sub('[0-9]+', '', re.sub(r'http\\S+', '', re.sub(cleanr, '', x))))\n",
    "#    #rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "#    #rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "#    text = text.apply(lambda x : x.lower().replace('\\n', ' ').strip())\n",
    "#    #data['content'] = data['content'].apply(lambda x: re.sub('[0-9]+', '', x))\n",
    "#    text = text.apply(lambda x: x[0:512])\n",
    "#    return text\n",
    "#X = data.loc[:20, ['symbol', 'content']]\n",
    "#X['content'] = X['content'].apply(lambda x: x[0:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf204108-cce2-42fa-95f3-db9332012bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPG</td>\n",
       "      <td>your morning coffee: // #earnings    costco (c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPG</td>\n",
       "      <td>tendiegenerator.java     import java.util.arra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IPG</td>\n",
       "      <td>j.p. morgan early look at the market – mon .. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IPG</td>\n",
       "      <td>wall street week ahead for the trading week be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IPG</td>\n",
       "      <td>your morning coffee [wednesday ..] #earnings  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212310</th>\n",
       "      <td>FIVE</td>\n",
       "      <td>explain to me like i'm five. is hmny really to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212311</th>\n",
       "      <td>FIVE</td>\n",
       "      <td>after holding for five years, selling $fb toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212312</th>\n",
       "      <td>FIVE</td>\n",
       "      <td>oracle just somehow jumped % in five minutes? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212313</th>\n",
       "      <td>FIVE</td>\n",
       "      <td>five of nine seems fine for usa mines &amp;#xb;  [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212314</th>\n",
       "      <td>FIVE</td>\n",
       "      <td>all five below locations | discount store, nov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212315 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       symbol                                            content\n",
       "0         IPG  your morning coffee: // #earnings    costco (c...\n",
       "1         IPG  tendiegenerator.java     import java.util.arra...\n",
       "2         IPG  j.p. morgan early look at the market – mon .. ...\n",
       "3         IPG  wall street week ahead for the trading week be...\n",
       "4         IPG  your morning coffee [wednesday ..] #earnings  ...\n",
       "...       ...                                                ...\n",
       "212310   FIVE  explain to me like i'm five. is hmny really to...\n",
       "212311   FIVE  after holding for five years, selling $fb toda...\n",
       "212312   FIVE  oracle just somehow jumped % in five minutes? ...\n",
       "212313   FIVE  five of nine seems fine for usa mines &#xb;  [...\n",
       "212314   FIVE  all five below locations | discount store, nov...\n",
       "\n",
       "[212315 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.loc[:, ['symbol', 'content']]\n",
    "X['content'] = X['content'].apply(lambda x: x[0:512])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81f05909-be90-4bf1-abc3-4090d8c50fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# The tokenization learns a dictionary that maps a token (integer) to each word\n",
    "# It can be done only on the train set - we are not supposed to know the test set!\n",
    "# This tokenization also lowercases your words, apply some filters, and so on - you can check the doc if you want\n",
    "text = tokenizer.fit_on_texts(X['content'])\n",
    "    \n",
    "# We apply the tokenization to the train and test set\n",
    "X_token = tokenizer.texts_to_sequences(X['content'])\n",
    "#X_test_token = tokenizer.texts_to_sequences(X_test)\n",
    "tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb82925e-c759-439d-9771-b307300ff17d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/lico/.local/lib/python3.10/site-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/lico/.local/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lico/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lico/.local/lib/python3.10/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lico/.local/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: requests in /home/lico/.local/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/lico/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /home/lico/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/lico/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/lico/.local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lico/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lico/.local/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cef624e1-bcf9-487d-90a8-ef93428dbcfd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [72], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#print(sentences)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m results\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1100\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1097\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1098\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1099\u001b[0m     )\n\u001b[0;32m-> 1100\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1025\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1024\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1025\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:182\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:462\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 462\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales.\",  \n",
    "             \"Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least EUR 4,000.\", \n",
    "             \"Raute reported a loss per share of EUR 0.86 for the first half of 2009 , against EPS of EUR 0.74 in the corresponding period of 2008.\", \n",
    "             ]\n",
    "sentences = list(X['content'])\n",
    "#print(sentences)\n",
    "results = nlp(sentences)\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e649b1a5-266f-4da4-a75f-7d5ea5677105",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.9995100498199463},\n",
       " {'label': 'neutral', 'score': 0.9997513890266418},\n",
       " {'label': 'neutral', 'score': 0.9959729313850403},\n",
       " {'label': 'positive', 'score': 0.9997192025184631},\n",
       " {'label': 'neutral', 'score': 0.9997091889381409},\n",
       " {'label': 'negative', 'score': 0.9953179359436035},\n",
       " {'label': 'neutral', 'score': 0.9971987009048462},\n",
       " {'label': 'neutral', 'score': 0.9984163045883179},\n",
       " {'label': 'neutral', 'score': 0.998609185218811},\n",
       " {'label': 'neutral', 'score': 0.9993522763252258},\n",
       " {'label': 'neutral', 'score': 0.9983896017074585},\n",
       " {'label': 'neutral', 'score': 0.9955641031265259},\n",
       " {'label': 'neutral', 'score': 0.9995648264884949},\n",
       " {'label': 'neutral', 'score': 0.5609238147735596},\n",
       " {'label': 'neutral', 'score': 0.9995728135108948},\n",
       " {'label': 'neutral', 'score': 0.9990052580833435},\n",
       " {'label': 'neutral', 'score': 0.9855387210845947},\n",
       " {'label': 'neutral', 'score': 0.9996201992034912},\n",
       " {'label': 'neutral', 'score': 0.9984922409057617},\n",
       " {'label': 'neutral', 'score': 0.9992837309837341},\n",
       " {'label': 'neutral', 'score': 0.9981162548065186},\n",
       " {'label': 'neutral', 'score': 0.9947094917297363},\n",
       " {'label': 'neutral', 'score': 0.9983035326004028},\n",
       " {'label': 'positive', 'score': 0.5678481459617615},\n",
       " {'label': 'neutral', 'score': 0.998663067817688},\n",
       " {'label': 'neutral', 'score': 0.9997209906578064},\n",
       " {'label': 'neutral', 'score': 0.999620795249939},\n",
       " {'label': 'neutral', 'score': 0.9987218976020813},\n",
       " {'label': 'neutral', 'score': 0.9895319938659668},\n",
       " {'label': 'positive', 'score': 0.9995786547660828},\n",
       " {'label': 'neutral', 'score': 0.9990798234939575},\n",
       " {'label': 'neutral', 'score': 0.9997655749320984},\n",
       " {'label': 'neutral', 'score': 0.9995096921920776},\n",
       " {'label': 'neutral', 'score': 0.9988000392913818},\n",
       " {'label': 'neutral', 'score': 0.9996552467346191},\n",
       " {'label': 'neutral', 'score': 0.9993632435798645},\n",
       " {'label': 'neutral', 'score': 0.999783456325531},\n",
       " {'label': 'neutral', 'score': 0.998730480670929},\n",
       " {'label': 'neutral', 'score': 0.997509241104126},\n",
       " {'label': 'positive', 'score': 0.999068558216095},\n",
       " {'label': 'neutral', 'score': 0.9981114864349365},\n",
       " {'label': 'neutral', 'score': 0.9993218183517456},\n",
       " {'label': 'negative', 'score': 0.7085356712341309},\n",
       " {'label': 'neutral', 'score': 0.9987443685531616},\n",
       " {'label': 'neutral', 'score': 0.999811589717865},\n",
       " {'label': 'neutral', 'score': 0.9409445524215698},\n",
       " {'label': 'neutral', 'score': 0.9997652173042297},\n",
       " {'label': 'neutral', 'score': 0.9248546361923218},\n",
       " {'label': 'neutral', 'score': 0.9996985197067261},\n",
       " {'label': 'neutral', 'score': 0.9995827078819275},\n",
       " {'label': 'neutral', 'score': 0.6579938530921936},\n",
       " {'label': 'neutral', 'score': 0.9995939135551453},\n",
       " {'label': 'neutral', 'score': 0.5969094038009644},\n",
       " {'label': 'positive', 'score': 0.9255427718162537},\n",
       " {'label': 'neutral', 'score': 0.9997555613517761},\n",
       " {'label': 'neutral', 'score': 0.9993506073951721},\n",
       " {'label': 'neutral', 'score': 0.9983944296836853},\n",
       " {'label': 'positive', 'score': 0.9997468590736389},\n",
       " {'label': 'neutral', 'score': 0.6853093504905701},\n",
       " {'label': 'neutral', 'score': 0.998354971408844},\n",
       " {'label': 'neutral', 'score': 0.9993519186973572},\n",
       " {'label': 'negative', 'score': 0.9950453042984009},\n",
       " {'label': 'neutral', 'score': 0.9818397164344788},\n",
       " {'label': 'neutral', 'score': 0.68459552526474},\n",
       " {'label': 'neutral', 'score': 0.9998291730880737},\n",
       " {'label': 'neutral', 'score': 0.9702928066253662},\n",
       " {'label': 'neutral', 'score': 0.9996540546417236},\n",
       " {'label': 'positive', 'score': 0.5982854962348938},\n",
       " {'label': 'neutral', 'score': 0.9989184141159058},\n",
       " {'label': 'neutral', 'score': 0.9995429515838623},\n",
       " {'label': 'neutral', 'score': 0.9947715401649475},\n",
       " {'label': 'positive', 'score': 0.9972035884857178},\n",
       " {'label': 'neutral', 'score': 0.5799522399902344},\n",
       " {'label': 'neutral', 'score': 0.9995980858802795},\n",
       " {'label': 'neutral', 'score': 0.9993769526481628},\n",
       " {'label': 'neutral', 'score': 0.9991855025291443},\n",
       " {'label': 'neutral', 'score': 0.9994714856147766},\n",
       " {'label': 'neutral', 'score': 0.925864577293396},\n",
       " {'label': 'neutral', 'score': 0.8912172317504883},\n",
       " {'label': 'neutral', 'score': 0.999638557434082},\n",
       " {'label': 'negative', 'score': 0.8541182279586792},\n",
       " {'label': 'neutral', 'score': 0.9938617944717407},\n",
       " {'label': 'neutral', 'score': 0.9932059049606323},\n",
       " {'label': 'neutral', 'score': 0.9979318380355835},\n",
       " {'label': 'neutral', 'score': 0.9996066689491272},\n",
       " {'label': 'neutral', 'score': 0.99974125623703},\n",
       " {'label': 'neutral', 'score': 0.9989390969276428},\n",
       " {'label': 'neutral', 'score': 0.9997078776359558},\n",
       " {'label': 'neutral', 'score': 0.9674107432365417},\n",
       " {'label': 'positive', 'score': 0.9997430443763733},\n",
       " {'label': 'positive', 'score': 0.9790275692939758},\n",
       " {'label': 'negative', 'score': 0.9962129592895508},\n",
       " {'label': 'positive', 'score': 0.9992914199829102},\n",
       " {'label': 'neutral', 'score': 0.9998302459716797},\n",
       " {'label': 'neutral', 'score': 0.9996658563613892},\n",
       " {'label': 'neutral', 'score': 0.9994354844093323},\n",
       " {'label': 'positive', 'score': 0.873741865158081},\n",
       " {'label': 'positive', 'score': 0.9991724491119385},\n",
       " {'label': 'positive', 'score': 0.9997201561927795},\n",
       " {'label': 'neutral', 'score': 0.9967647790908813},\n",
       " {'label': 'neutral', 'score': 0.9983184337615967},\n",
       " {'label': 'neutral', 'score': 0.7320762276649475},\n",
       " {'label': 'positive', 'score': 0.9998351335525513},\n",
       " {'label': 'neutral', 'score': 0.9967958331108093},\n",
       " {'label': 'neutral', 'score': 0.9989318251609802},\n",
       " {'label': 'neutral', 'score': 0.9961020946502686},\n",
       " {'label': 'neutral', 'score': 0.8305359482765198},\n",
       " {'label': 'neutral', 'score': 0.9988829493522644},\n",
       " {'label': 'neutral', 'score': 0.9997041821479797},\n",
       " {'label': 'neutral', 'score': 0.9947800636291504},\n",
       " {'label': 'negative', 'score': 0.6132538318634033},\n",
       " {'label': 'neutral', 'score': 0.9997296929359436},\n",
       " {'label': 'neutral', 'score': 0.9993902444839478},\n",
       " {'label': 'negative', 'score': 0.8626213669776917},\n",
       " {'label': 'neutral', 'score': 0.9979725480079651},\n",
       " {'label': 'positive', 'score': 0.9998225569725037},\n",
       " {'label': 'neutral', 'score': 0.9993785619735718},\n",
       " {'label': 'neutral', 'score': 0.9998099207878113},\n",
       " {'label': 'neutral', 'score': 0.9997901320457458},\n",
       " {'label': 'neutral', 'score': 0.9989935755729675},\n",
       " {'label': 'neutral', 'score': 0.9962894916534424},\n",
       " {'label': 'positive', 'score': 0.9093017578125},\n",
       " {'label': 'neutral', 'score': 0.9996705055236816},\n",
       " {'label': 'neutral', 'score': 0.9969426989555359},\n",
       " {'label': 'neutral', 'score': 0.9991744160652161},\n",
       " {'label': 'neutral', 'score': 0.9995960593223572},\n",
       " {'label': 'neutral', 'score': 0.9996114373207092},\n",
       " {'label': 'neutral', 'score': 0.9993125200271606},\n",
       " {'label': 'neutral', 'score': 0.9986822009086609},\n",
       " {'label': 'neutral', 'score': 0.9989399313926697},\n",
       " {'label': 'neutral', 'score': 0.9994696974754333},\n",
       " {'label': 'positive', 'score': 0.9985771179199219},\n",
       " {'label': 'neutral', 'score': 0.9175559282302856},\n",
       " {'label': 'neutral', 'score': 0.9938778281211853},\n",
       " {'label': 'neutral', 'score': 0.9997108578681946},\n",
       " {'label': 'neutral', 'score': 0.9987679123878479},\n",
       " {'label': 'neutral', 'score': 0.9907834529876709},\n",
       " {'label': 'neutral', 'score': 0.9911683797836304},\n",
       " {'label': 'neutral', 'score': 0.9997233748435974},\n",
       " {'label': 'neutral', 'score': 0.994976282119751},\n",
       " {'label': 'neutral', 'score': 0.9997672438621521},\n",
       " {'label': 'positive', 'score': 0.5762929916381836},\n",
       " {'label': 'positive', 'score': 0.9996260404586792},\n",
       " {'label': 'negative', 'score': 0.9911674857139587},\n",
       " {'label': 'neutral', 'score': 0.9993011951446533},\n",
       " {'label': 'neutral', 'score': 0.999103307723999},\n",
       " {'label': 'neutral', 'score': 0.9973152279853821},\n",
       " {'label': 'neutral', 'score': 0.9983370304107666},\n",
       " {'label': 'neutral', 'score': 0.9761278033256531},\n",
       " {'label': 'neutral', 'score': 0.917941153049469},\n",
       " {'label': 'neutral', 'score': 0.9902607798576355},\n",
       " {'label': 'neutral', 'score': 0.9989677667617798},\n",
       " {'label': 'neutral', 'score': 0.9996482133865356},\n",
       " {'label': 'neutral', 'score': 0.9916632175445557},\n",
       " {'label': 'neutral', 'score': 0.9401246309280396},\n",
       " {'label': 'negative', 'score': 0.9989537000656128},\n",
       " {'label': 'positive', 'score': 0.9984645843505859},\n",
       " {'label': 'positive', 'score': 0.9991939663887024},\n",
       " {'label': 'neutral', 'score': 0.999703586101532},\n",
       " {'label': 'neutral', 'score': 0.9881019592285156},\n",
       " {'label': 'neutral', 'score': 0.9996050000190735},\n",
       " {'label': 'positive', 'score': 0.5058162212371826},\n",
       " {'label': 'neutral', 'score': 0.9941214919090271},\n",
       " {'label': 'neutral', 'score': 0.9703989624977112},\n",
       " {'label': 'neutral', 'score': 0.9997614026069641},\n",
       " {'label': 'neutral', 'score': 0.9995341300964355},\n",
       " {'label': 'neutral', 'score': 0.8144822120666504},\n",
       " {'label': 'neutral', 'score': 0.999657392501831},\n",
       " {'label': 'neutral', 'score': 0.9996044039726257},\n",
       " {'label': 'positive', 'score': 0.998297393321991},\n",
       " {'label': 'neutral', 'score': 0.9983365535736084},\n",
       " {'label': 'neutral', 'score': 0.9993785619735718},\n",
       " {'label': 'neutral', 'score': 0.999686598777771},\n",
       " {'label': 'neutral', 'score': 0.9992864727973938},\n",
       " {'label': 'neutral', 'score': 0.9974743723869324},\n",
       " {'label': 'neutral', 'score': 0.9990888833999634},\n",
       " {'label': 'neutral', 'score': 0.9975866079330444},\n",
       " {'label': 'neutral', 'score': 0.9932061433792114},\n",
       " {'label': 'neutral', 'score': 0.9918197393417358},\n",
       " {'label': 'neutral', 'score': 0.9989481568336487},\n",
       " {'label': 'neutral', 'score': 0.9995299577713013},\n",
       " {'label': 'neutral', 'score': 0.9983013868331909},\n",
       " {'label': 'neutral', 'score': 0.5895681381225586},\n",
       " {'label': 'neutral', 'score': 0.9995830655097961},\n",
       " {'label': 'neutral', 'score': 0.936492383480072},\n",
       " {'label': 'neutral', 'score': 0.9997929930686951},\n",
       " {'label': 'neutral', 'score': 0.9990597367286682},\n",
       " {'label': 'neutral', 'score': 0.9847204685211182},\n",
       " {'label': 'neutral', 'score': 0.9991752505302429},\n",
       " {'label': 'neutral', 'score': 0.9989497065544128},\n",
       " {'label': 'neutral', 'score': 0.994889497756958},\n",
       " {'label': 'neutral', 'score': 0.9988619089126587},\n",
       " {'label': 'neutral', 'score': 0.9946904182434082},\n",
       " {'label': 'neutral', 'score': 0.9993205070495605},\n",
       " {'label': 'neutral', 'score': 0.9647595882415771},\n",
       " {'label': 'neutral', 'score': 0.9211152791976929},\n",
       " {'label': 'neutral', 'score': 0.9905663132667542},\n",
       " {'label': 'neutral', 'score': 0.997588038444519},\n",
       " {'label': 'positive', 'score': 0.5430492758750916},\n",
       " {'label': 'neutral', 'score': 0.997238039970398},\n",
       " {'label': 'neutral', 'score': 0.9996410608291626},\n",
       " {'label': 'neutral', 'score': 0.9996645450592041},\n",
       " {'label': 'neutral', 'score': 0.9990370273590088},\n",
       " {'label': 'positive', 'score': 0.8138342499732971},\n",
       " {'label': 'neutral', 'score': 0.9994801878929138},\n",
       " {'label': 'neutral', 'score': 0.9585433602333069},\n",
       " {'label': 'neutral', 'score': 0.9993507266044617},\n",
       " {'label': 'neutral', 'score': 0.9995514750480652},\n",
       " {'label': 'neutral', 'score': 0.9689457416534424},\n",
       " {'label': 'neutral', 'score': 0.9992353916168213},\n",
       " {'label': 'neutral', 'score': 0.9996293783187866},\n",
       " {'label': 'neutral', 'score': 0.997043788433075},\n",
       " {'label': 'negative', 'score': 0.9990585446357727},\n",
       " {'label': 'positive', 'score': 0.9724524021148682},\n",
       " {'label': 'neutral', 'score': 0.9967086315155029},\n",
       " {'label': 'neutral', 'score': 0.9994157552719116},\n",
       " {'label': 'neutral', 'score': 0.99688321352005},\n",
       " {'label': 'neutral', 'score': 0.9993783235549927},\n",
       " {'label': 'neutral', 'score': 0.9994810223579407},\n",
       " {'label': 'neutral', 'score': 0.9995067119598389},\n",
       " {'label': 'neutral', 'score': 0.9967613816261292},\n",
       " {'label': 'neutral', 'score': 0.9925743341445923},\n",
       " {'label': 'neutral', 'score': 0.997509241104126},\n",
       " {'label': 'neutral', 'score': 0.9994111061096191},\n",
       " {'label': 'neutral', 'score': 0.9990910291671753},\n",
       " {'label': 'neutral', 'score': 0.9422899484634399},\n",
       " {'label': 'neutral', 'score': 0.997506320476532},\n",
       " {'label': 'neutral', 'score': 0.9977784752845764},\n",
       " {'label': 'negative', 'score': 0.9990369081497192},\n",
       " {'label': 'negative', 'score': 0.9976612329483032},\n",
       " {'label': 'negative', 'score': 0.9842551946640015},\n",
       " {'label': 'neutral', 'score': 0.9996088147163391},\n",
       " {'label': 'neutral', 'score': 0.9798704385757446},\n",
       " {'label': 'neutral', 'score': 0.9995806813240051},\n",
       " {'label': 'neutral', 'score': 0.9977971315383911},\n",
       " {'label': 'neutral', 'score': 0.9916342496871948},\n",
       " {'label': 'neutral', 'score': 0.9891185760498047},\n",
       " {'label': 'neutral', 'score': 0.9991381168365479},\n",
       " {'label': 'negative', 'score': 0.9996237754821777},\n",
       " {'label': 'positive', 'score': 0.9990233182907104},\n",
       " {'label': 'neutral', 'score': 0.9995488524436951},\n",
       " {'label': 'neutral', 'score': 0.9995880722999573},\n",
       " {'label': 'neutral', 'score': 0.9971942901611328},\n",
       " {'label': 'neutral', 'score': 0.9997722506523132},\n",
       " {'label': 'neutral', 'score': 0.9989355206489563},\n",
       " {'label': 'positive', 'score': 0.9917142987251282},\n",
       " {'label': 'neutral', 'score': 0.9997515082359314},\n",
       " {'label': 'neutral', 'score': 0.9981156587600708},\n",
       " {'label': 'neutral', 'score': 0.9842755198478699},\n",
       " {'label': 'neutral', 'score': 0.7339245676994324},\n",
       " {'label': 'neutral', 'score': 0.9985425472259521},\n",
       " {'label': 'neutral', 'score': 0.9993657469749451},\n",
       " {'label': 'positive', 'score': 0.9998421669006348},\n",
       " {'label': 'neutral', 'score': 0.9993492960929871},\n",
       " {'label': 'neutral', 'score': 0.9996155500411987},\n",
       " {'label': 'neutral', 'score': 0.9994643330574036},\n",
       " {'label': 'neutral', 'score': 0.9086700081825256},\n",
       " {'label': 'neutral', 'score': 0.999504804611206},\n",
       " {'label': 'neutral', 'score': 0.9994266033172607},\n",
       " {'label': 'neutral', 'score': 0.9995494484901428},\n",
       " {'label': 'neutral', 'score': 0.9991657733917236},\n",
       " {'label': 'neutral', 'score': 0.99901282787323},\n",
       " {'label': 'positive', 'score': 0.9995248317718506},\n",
       " {'label': 'neutral', 'score': 0.9986594915390015},\n",
       " {'label': 'positive', 'score': 0.7398331761360168},\n",
       " {'label': 'neutral', 'score': 0.9353569149971008},\n",
       " {'label': 'neutral', 'score': 0.9996793270111084},\n",
       " {'label': 'neutral', 'score': 0.9997236132621765},\n",
       " {'label': 'neutral', 'score': 0.9992976188659668},\n",
       " {'label': 'neutral', 'score': 0.9996621608734131},\n",
       " {'label': 'neutral', 'score': 0.9980564713478088},\n",
       " {'label': 'neutral', 'score': 0.9994434714317322},\n",
       " {'label': 'neutral', 'score': 0.9992241859436035},\n",
       " {'label': 'positive', 'score': 0.7688261866569519},\n",
       " {'label': 'neutral', 'score': 0.9703989624977112},\n",
       " {'label': 'positive', 'score': 0.9423528909683228},\n",
       " {'label': 'neutral', 'score': 0.9996102452278137},\n",
       " {'label': 'neutral', 'score': 0.962236762046814},\n",
       " {'label': 'positive', 'score': 0.999790370464325},\n",
       " {'label': 'negative', 'score': 0.45439326763153076},\n",
       " {'label': 'neutral', 'score': 0.9958826303482056},\n",
       " {'label': 'negative', 'score': 0.9986340403556824},\n",
       " {'label': 'negative', 'score': 0.8324534893035889},\n",
       " {'label': 'neutral', 'score': 0.9667549133300781},\n",
       " {'label': 'neutral', 'score': 0.9977200627326965},\n",
       " {'label': 'neutral', 'score': 0.9897340536117554},\n",
       " {'label': 'neutral', 'score': 0.9992017149925232},\n",
       " {'label': 'neutral', 'score': 0.9994739890098572},\n",
       " {'label': 'neutral', 'score': 0.9908148646354675},\n",
       " {'label': 'neutral', 'score': 0.9985290765762329},\n",
       " {'label': 'positive', 'score': 0.9473626017570496},\n",
       " {'label': 'neutral', 'score': 0.9768162965774536},\n",
       " {'label': 'neutral', 'score': 0.9995965361595154},\n",
       " {'label': 'neutral', 'score': 0.9996538162231445},\n",
       " {'label': 'neutral', 'score': 0.9994118213653564},\n",
       " {'label': 'neutral', 'score': 0.9996552467346191},\n",
       " {'label': 'neutral', 'score': 0.9982665181159973},\n",
       " {'label': 'neutral', 'score': 0.9952074885368347},\n",
       " {'label': 'negative', 'score': 0.9368064403533936},\n",
       " {'label': 'positive', 'score': 0.9997758269309998},\n",
       " {'label': 'negative', 'score': 0.998828113079071},\n",
       " {'label': 'positive', 'score': 0.8310477137565613},\n",
       " {'label': 'positive', 'score': 0.5874528884887695},\n",
       " {'label': 'positive', 'score': 0.5235718488693237},\n",
       " {'label': 'neutral', 'score': 0.9973064661026001},\n",
       " {'label': 'neutral', 'score': 0.9986951947212219},\n",
       " {'label': 'neutral', 'score': 0.9987466335296631},\n",
       " {'label': 'neutral', 'score': 0.9995594620704651},\n",
       " {'label': 'neutral', 'score': 0.9988008737564087},\n",
       " {'label': 'neutral', 'score': 0.9672702550888062},\n",
       " {'label': 'neutral', 'score': 0.9745599031448364},\n",
       " {'label': 'neutral', 'score': 0.9996131062507629},\n",
       " {'label': 'neutral', 'score': 0.9997444748878479},\n",
       " {'label': 'positive', 'score': 0.9997662901878357},\n",
       " {'label': 'neutral', 'score': 0.9993672966957092},\n",
       " {'label': 'neutral', 'score': 0.9939534068107605},\n",
       " {'label': 'neutral', 'score': 0.9987165927886963},\n",
       " {'label': 'positive', 'score': 0.9997708201408386},\n",
       " {'label': 'neutral', 'score': 0.9430402517318726},\n",
       " {'label': 'neutral', 'score': 0.7989447712898254},\n",
       " {'label': 'neutral', 'score': 0.9997426867485046},\n",
       " {'label': 'neutral', 'score': 0.9995747208595276},\n",
       " {'label': 'neutral', 'score': 0.9971942901611328},\n",
       " {'label': 'neutral', 'score': 0.9996224641799927},\n",
       " {'label': 'neutral', 'score': 0.9849855899810791},\n",
       " {'label': 'neutral', 'score': 0.9993131160736084},\n",
       " {'label': 'neutral', 'score': 0.9987536668777466},\n",
       " {'label': 'neutral', 'score': 0.9997593760490417},\n",
       " {'label': 'neutral', 'score': 0.9956998825073242},\n",
       " {'label': 'neutral', 'score': 0.999618649482727},\n",
       " {'label': 'neutral', 'score': 0.9994677901268005},\n",
       " {'label': 'neutral', 'score': 0.9924806356430054},\n",
       " {'label': 'neutral', 'score': 0.9994838237762451},\n",
       " {'label': 'neutral', 'score': 0.9994199275970459},\n",
       " {'label': 'neutral', 'score': 0.9879186749458313},\n",
       " {'label': 'neutral', 'score': 0.6235857009887695},\n",
       " {'label': 'neutral', 'score': 0.9994235038757324},\n",
       " {'label': 'neutral', 'score': 0.9636386036872864},\n",
       " {'label': 'neutral', 'score': 0.9592708945274353},\n",
       " {'label': 'neutral', 'score': 0.9988276362419128},\n",
       " {'label': 'neutral', 'score': 0.990202009677887},\n",
       " {'label': 'neutral', 'score': 0.999701201915741},\n",
       " {'label': 'neutral', 'score': 0.8725049495697021},\n",
       " {'label': 'neutral', 'score': 0.9983553290367126},\n",
       " {'label': 'neutral', 'score': 0.9994539618492126},\n",
       " {'label': 'positive', 'score': 0.9996974468231201},\n",
       " {'label': 'neutral', 'score': 0.9987383484840393},\n",
       " {'label': 'positive', 'score': 0.9998526573181152},\n",
       " {'label': 'neutral', 'score': 0.9995855689048767},\n",
       " {'label': 'negative', 'score': 0.9548447132110596},\n",
       " {'label': 'positive', 'score': 0.9997383952140808},\n",
       " {'label': 'neutral', 'score': 0.9933944344520569},\n",
       " {'label': 'neutral', 'score': 0.9832974672317505},\n",
       " {'label': 'positive', 'score': 0.9998067021369934},\n",
       " {'label': 'neutral', 'score': 0.9997491240501404},\n",
       " {'label': 'negative', 'score': 0.982240617275238},\n",
       " {'label': 'neutral', 'score': 0.9778037667274475},\n",
       " {'label': 'neutral', 'score': 0.5502123236656189},\n",
       " {'label': 'neutral', 'score': 0.9990707039833069},\n",
       " {'label': 'neutral', 'score': 0.999214768409729},\n",
       " {'label': 'neutral', 'score': 0.9988573789596558},\n",
       " {'label': 'neutral', 'score': 0.9994456171989441},\n",
       " {'label': 'neutral', 'score': 0.9993360638618469},\n",
       " {'label': 'neutral', 'score': 0.9998341798782349},\n",
       " {'label': 'neutral', 'score': 0.9997139573097229},\n",
       " {'label': 'neutral', 'score': 0.9979810118675232},\n",
       " {'label': 'positive', 'score': 0.9995012283325195},\n",
       " {'label': 'neutral', 'score': 0.9994543194770813},\n",
       " {'label': 'neutral', 'score': 0.9978876709938049},\n",
       " {'label': 'neutral', 'score': 0.9818601012229919},\n",
       " {'label': 'positive', 'score': 0.9998151659965515},\n",
       " {'label': 'neutral', 'score': 0.9510028958320618},\n",
       " {'label': 'neutral', 'score': 0.9994295239448547},\n",
       " {'label': 'neutral', 'score': 0.9932790398597717},\n",
       " {'label': 'neutral', 'score': 0.9978825449943542},\n",
       " {'label': 'neutral', 'score': 0.9958848357200623},\n",
       " {'label': 'neutral', 'score': 0.9954392313957214},\n",
       " {'label': 'neutral', 'score': 0.9969520568847656},\n",
       " {'label': 'neutral', 'score': 0.9865379333496094},\n",
       " {'label': 'neutral', 'score': 0.9994089603424072},\n",
       " {'label': 'neutral', 'score': 0.9043038487434387},\n",
       " {'label': 'neutral', 'score': 0.9997475743293762},\n",
       " {'label': 'neutral', 'score': 0.9978628754615784},\n",
       " {'label': 'neutral', 'score': 0.999369204044342},\n",
       " {'label': 'neutral', 'score': 0.9993434548377991},\n",
       " {'label': 'neutral', 'score': 0.9996658563613892},\n",
       " {'label': 'neutral', 'score': 0.9996088147163391},\n",
       " {'label': 'neutral', 'score': 0.9452200531959534},\n",
       " {'label': 'neutral', 'score': 0.9994971752166748},\n",
       " {'label': 'neutral', 'score': 0.9995425939559937},\n",
       " {'label': 'positive', 'score': 0.9998452663421631},\n",
       " {'label': 'positive', 'score': 0.6090438365936279},\n",
       " {'label': 'positive', 'score': 0.9985087513923645},\n",
       " {'label': 'neutral', 'score': 0.9994171857833862},\n",
       " {'label': 'negative', 'score': 0.927432119846344},\n",
       " {'label': 'positive', 'score': 0.9992352724075317},\n",
       " {'label': 'neutral', 'score': 0.6779080033302307},\n",
       " {'label': 'neutral', 'score': 0.9995771050453186},\n",
       " {'label': 'positive', 'score': 0.6933097243309021},\n",
       " {'label': 'positive', 'score': 0.9998453855514526},\n",
       " {'label': 'neutral', 'score': 0.9981306195259094},\n",
       " {'label': 'neutral', 'score': 0.9978328347206116},\n",
       " {'label': 'neutral', 'score': 0.9739215970039368},\n",
       " {'label': 'neutral', 'score': 0.9778565764427185},\n",
       " {'label': 'positive', 'score': 0.9978074431419373}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Appl\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "X = pd.read_csv('../raw_data/reddit_AAPL_msr.csv')\n",
    "X = X.loc[:, ['symbol', 'content', 'created_at', 'score']]\n",
    "#X['content'] = X['content'].apply(lambda x: x[0:512])\n",
    "X['content'] = clean_data(X['content'])\n",
    "X\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentences = list(X['content'])\n",
    "#print(sentences)\n",
    "results = nlp(sentences)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50c56bd4-cb10-4b32-907d-d3b5925f1473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          score\n",
      "label          \n",
      "negative     22\n",
      "neutral     328\n",
      "positive     55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "score    0.809877\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume = pd.DataFrame(results).groupby('label').count()\n",
    "print(resume)\n",
    "resume.reset_index()['score'][1]/resume.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9e275d4b-74c0-4e8a-8a35-701d83e75990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_349065/1981893162.py:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  X.loc[:, 'sentiment'] = d['label'].apply(lambda x: sentiment[x])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>content</th>\n",
       "      <th>created_at</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>bears feasting on aapl next week confirmed</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>21050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>someone purchased $. billion worth of $aapl st...</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>20769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>apple started wwiii so that phones would be de...</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>15905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>aapl's innovative product strategy</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>11880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>warren buffett: ‘if someone offered you $, to ...</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>10883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>aapl earnings release bear case : msft missed ...</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>aapl loss porn</td>\n",
       "      <td>2020-09-17</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>the stock market should not bottom until $aapl...</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>aapl yolo update. i was down k just a few week...</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>$aapl up almost % since february, thoughts on ...</td>\n",
       "      <td>2018-06-05</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    symbol                                            content  created_at  \\\n",
       "0     AAPL         bears feasting on aapl next week confirmed  2020-09-26   \n",
       "1     AAPL  someone purchased $. billion worth of $aapl st...  2022-12-05   \n",
       "2     AAPL  apple started wwiii so that phones would be de...  2020-01-04   \n",
       "3     AAPL                 aapl's innovative product strategy  2019-01-03   \n",
       "4     AAPL  warren buffett: ‘if someone offered you $, to ...  2023-04-12   \n",
       "..     ...                                                ...         ...   \n",
       "400   AAPL  aapl earnings release bear case : msft missed ...  2023-05-01   \n",
       "401   AAPL                                     aapl loss porn  2020-09-17   \n",
       "402   AAPL  the stock market should not bottom until $aapl...  2022-12-23   \n",
       "403   AAPL  aapl yolo update. i was down k just a few week...  2022-12-28   \n",
       "404   AAPL  $aapl up almost % since february, thoughts on ...  2018-06-05   \n",
       "\n",
       "     score  sentiment  \n",
       "0    21050          0  \n",
       "1    20769          0  \n",
       "2    15905          0  \n",
       "3    11880          1  \n",
       "4    10883          0  \n",
       "..     ...        ...  \n",
       "400    106          0  \n",
       "401    105          0  \n",
       "402    104          0  \n",
       "403    103          0  \n",
       "404    102          1  \n",
       "\n",
       "[405 rows x 5 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = {\n",
    "    'negative': -1,\n",
    "    'neutral': 0,\n",
    "    'positive': 1,\n",
    "}\n",
    "d = pd.DataFrame(results)\n",
    "X.loc[:, 'sentiment'] = d['label'].apply(lambda x: sentiment[x])\n",
    "X[['symbol', 'score', 'created_at', 'sentiment']]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e46b8e99-68c5-49e7-8700-5ecdcaa66eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('../raw_data/reddit_sentimental_AAPL.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fc6781b9-fb19-4479-85c2-133b8a5b17ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) negative 0.7236\n",
      "2) neutral 0.2287\n",
      "3) positive 0.0477\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "text = \"Covid cases are increasing fast!\"\n",
    "def sentiment_predict(text):\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    #print(scores)\n",
    "    #print(pd.DataFrame(score))\n",
    "    result = config.id2label[pd.DataFrame(scores).idxmax()[0]]\n",
    "    return result\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "# text = \"Covid cases are increasing fast!\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "076b1bf7-eef4-4910-a2a6-3f96fdccda59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = pd.DataFrame(sentiment(X['content'][0]))\n",
    "config.id2label[score.idxmax()[0]]\n",
    "rr = X['content'].apply(lambda x: sentiment_predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eed33368-43c9-4a41-bcdc-415d92116696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sum\n",
      "content      \n",
      "negative   88\n",
      "neutral   209\n",
      "positive  108\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>content</th>\n",
       "      <th>created_at</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>bears feasting on aapl next week confirmed</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>21050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>someone purchased $. billion worth of $aapl st...</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>20769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>apple started wwiii so that phones would be de...</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>15905</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>aapl's innovative product strategy</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>11880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>warren buffett: ‘if someone offered you $, to ...</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>10883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>chinese foxconn workers facing off against pol...</td>\n",
       "      <td>2022-11-24</td>\n",
       "      <td>10473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>short $aapl</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>8198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>i trade aapl for a living my nephew told me ab...</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>8122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>meta/facebook stock crashes -% ah after earnin...</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>7812</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>loonnngg aapl</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>7383</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>long $aapl</td>\n",
       "      <td>2019-06-05</td>\n",
       "      <td>7318</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>it's not just me right? for the past month i'v...</td>\n",
       "      <td>2021-03-24</td>\n",
       "      <td>6953</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>***google survey update*** gme ownership w/ $a...</td>\n",
       "      <td>2021-08-04</td>\n",
       "      <td>6881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>their collateral is shrinking, they are bleedi...</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>6800</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>good thing $aapl makes up % of the s&amp;p :</td>\n",
       "      <td>2023-06-06</td>\n",
       "      <td>6646</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>to all of you who are red this year while the ...</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>5854</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>feel bad about missing a play? be inspired by ...</td>\n",
       "      <td>2023-05-09</td>\n",
       "      <td>5731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>aapl : the only dd you'll ever need</td>\n",
       "      <td>2023-06-06</td>\n",
       "      <td>5670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>beware of what amc shorts are holding! i under...</td>\n",
       "      <td>2021-06-03</td>\n",
       "      <td>5366</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>i tried to predict the stock market every day ...</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>5274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>on this day  years ago, barack obama almost pe...</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>5188</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>tim cook receives $m stock award as aapl meets...</td>\n",
       "      <td>2019-08-28</td>\n",
       "      <td>4814</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>for the youth, just broad strokes what i've le...</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>4642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>i’m convinced the airpods max active noise can...</td>\n",
       "      <td>2022-09-24</td>\n",
       "      <td>4620</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   symbol                                            content  created_at  \\\n",
       "0    AAPL         bears feasting on aapl next week confirmed  2020-09-26   \n",
       "1    AAPL  someone purchased $. billion worth of $aapl st...  2022-12-05   \n",
       "2    AAPL  apple started wwiii so that phones would be de...  2020-01-04   \n",
       "3    AAPL                 aapl's innovative product strategy  2019-01-03   \n",
       "4    AAPL  warren buffett: ‘if someone offered you $, to ...  2023-04-12   \n",
       "5    AAPL  chinese foxconn workers facing off against pol...  2022-11-24   \n",
       "6    AAPL                                        short $aapl  2019-01-29   \n",
       "7    AAPL  i trade aapl for a living my nephew told me ab...  2020-10-14   \n",
       "8    AAPL  meta/facebook stock crashes -% ah after earnin...  2022-02-02   \n",
       "9    AAPL                                      loonnngg aapl  2018-10-13   \n",
       "10   AAPL                                         long $aapl  2019-06-05   \n",
       "11   AAPL  it's not just me right? for the past month i'v...  2021-03-24   \n",
       "12   AAPL  ***google survey update*** gme ownership w/ $a...  2021-08-04   \n",
       "13   AAPL  their collateral is shrinking, they are bleedi...  2022-01-31   \n",
       "14   AAPL           good thing $aapl makes up % of the s&p :  2023-06-06   \n",
       "15   AAPL  to all of you who are red this year while the ...  2021-12-29   \n",
       "16   AAPL  feel bad about missing a play? be inspired by ...  2023-05-09   \n",
       "17   AAPL                aapl : the only dd you'll ever need  2023-06-06   \n",
       "18   AAPL  beware of what amc shorts are holding! i under...  2021-06-03   \n",
       "19   AAPL  i tried to predict the stock market every day ...  2021-04-01   \n",
       "20   AAPL  on this day  years ago, barack obama almost pe...  2022-03-03   \n",
       "21   AAPL  tim cook receives $m stock award as aapl meets...  2019-08-28   \n",
       "22   AAPL  for the youth, just broad strokes what i've le...  2021-04-07   \n",
       "23   AAPL  i’m convinced the airpods max active noise can...  2022-09-24   \n",
       "\n",
       "    score  sentiment  \n",
       "0   21050          0  \n",
       "1   20769          0  \n",
       "2   15905         -1  \n",
       "3   11880          1  \n",
       "4   10883          0  \n",
       "5   10473          0  \n",
       "6    8198          0  \n",
       "7    8122          1  \n",
       "8    7812         -1  \n",
       "9    7383          0  \n",
       "10   7318          0  \n",
       "11   6953         -1  \n",
       "12   6881          1  \n",
       "13   6800         -1  \n",
       "14   6646          1  \n",
       "15   5854         -1  \n",
       "16   5731          0  \n",
       "17   5670          0  \n",
       "18   5366          0  \n",
       "19   5274          0  \n",
       "20   5188          1  \n",
       "21   4814          1  \n",
       "22   4642          0  \n",
       "23   4620         -1  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(columns=['label', 'sum']) \n",
    "a = pd.DataFrame(rr, columns=['content', 'sum'])\n",
    "a['sum'] = 1\n",
    "print(a.groupby('content').count())\n",
    "a['content']\n",
    "X['sentiment'] = a['content'].apply(lambda x: sentiment[x])\n",
    "X.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a915be03-0fa1-4201-a2a4-ec0bdddb2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[23, 'content']\n",
    "X.to_csv('../raw_data/reddit_sentimental_AAPL.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d34c0ab-0771-42ca-b26a-856d5c4aa8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'someone purchased $. billion worth of $aapl stock on friday at the close😮'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[1, 'content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a73bdef-df3e-47cb-85ac-d26edc15f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "openai.api_key = os.environ[\"API_KEY\"]\n",
    "\n",
    "def get_sentiment(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=f\"Perform sentiment analysis on the following text:\\n{text}\\n respond with a single value between -1 and 1. I.e. if the sentiment is extremely positive, respond with a value such as 0.99. If somewhat negative respond with a value such as -0.63. Only respond with a single value.\",\n",
    "        temperature=0,\n",
    "        max_tokens=7,\n",
    "       # top_p=1,\n",
    "       # frequency_penalty=0,\n",
    "       # presence_penalty=0,\n",
    "       # stop=[\"\\n\"]\n",
    "    )\n",
    "\n",
    "    sentiment = response.choices[0].text.strip()\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3f795cb-ceb3-44ac-a01d-393098f3e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['sentiment'] = X['content'].apply(lambda x: get_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fc8e782-ae06-4914-a822-760b5c2bdd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X\n",
    "X.to_csv('../raw_data/reddit_sentimental_AAPL.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
